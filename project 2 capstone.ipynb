{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad243f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfc in position 5460: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Check each file individually\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[0;32m---> 39\u001b[0m     check_csv_file(path)\n",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m, in \u001b[0;36mcheck_csv_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_csv_file\u001b[39m(file_path):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Print basic info\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xfc in position 5460: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def check_csv_file(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Print basic info\n",
    "    print(f\"\\n{'='*40}\\nAnalyzing {os.path.basename(file_path)}\\n{'='*40}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    print(\"\\nDuplicate Rows:\")\n",
    "    print(df.duplicated().sum())\n",
    "    \n",
    "    # Check the data types of each column\n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Display the first few rows of the dataframe\n",
    "    print(\"\\nFirst Few Rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "# Paths for each file\n",
    "file_paths = [\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Customers.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Data_Dictionary.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Products.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Sales.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Stores.csv\"\n",
    "]\n",
    "\n",
    "# Check each file individually\n",
    "for path in file_paths:\n",
    "    check_csv_file(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d676453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading /Users/shadivaz/Desktop/PROJECT 2/Customers.csv with utf-8 encoding: 'utf-8' codec can't decode byte 0xf6 in position 7: invalid start byte\n",
      "\n",
      "========================================\n",
      "Analyzing Customers.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "CustomerKey     0\n",
      "Gender          0\n",
      "Name            0\n",
      "City            0\n",
      "State Code     10\n",
      "State           0\n",
      "Zip Code        0\n",
      "Country         0\n",
      "Continent       0\n",
      "Birthday        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "CustomerKey     int64\n",
      "Gender         object\n",
      "Name           object\n",
      "City           object\n",
      "State Code     object\n",
      "State          object\n",
      "Zip Code       object\n",
      "Country        object\n",
      "Continent      object\n",
      "Birthday       object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    South Australia     5523  Australia  Australia    7/3/1939  \n",
      "1  Western Australia     6522  Australia  Australia   9/27/1979  \n",
      "2           Victoria     3380  Australia  Australia   5/26/1947  \n",
      "3    South Australia     5223  Australia  Australia   9/17/1957  \n",
      "4           Victoria     3698  Australia  Australia  11/19/1965  \n",
      "\n",
      "========================================\n",
      "Analyzing Customers.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "CustomerKey     0\n",
      "Gender          0\n",
      "Name            0\n",
      "City            0\n",
      "State Code     10\n",
      "State           0\n",
      "Zip Code        0\n",
      "Country         0\n",
      "Continent       0\n",
      "Birthday        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "CustomerKey     int64\n",
      "Gender         object\n",
      "Name           object\n",
      "City           object\n",
      "State Code     object\n",
      "State          object\n",
      "Zip Code       object\n",
      "Country        object\n",
      "Continent      object\n",
      "Birthday       object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    South Australia     5523  Australia  Australia    7/3/1939  \n",
      "1  Western Australia     6522  Australia  Australia   9/27/1979  \n",
      "2           Victoria     3380  Australia  Australia   5/26/1947  \n",
      "3    South Australia     5223  Australia  Australia   9/17/1957  \n",
      "4           Victoria     3698  Australia  Australia  11/19/1965  \n",
      "\n",
      "========================================\n",
      "Analyzing Customers.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "CustomerKey     0\n",
      "Gender          0\n",
      "Name            0\n",
      "City            0\n",
      "State Code     10\n",
      "State           0\n",
      "Zip Code        0\n",
      "Country         0\n",
      "Continent       0\n",
      "Birthday        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "CustomerKey     int64\n",
      "Gender         object\n",
      "Name           object\n",
      "City           object\n",
      "State Code     object\n",
      "State          object\n",
      "Zip Code       object\n",
      "Country        object\n",
      "Continent      object\n",
      "Birthday       object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    South Australia     5523  Australia  Australia    7/3/1939  \n",
      "1  Western Australia     6522  Australia  Australia   9/27/1979  \n",
      "2           Victoria     3380  Australia  Australia   5/26/1947  \n",
      "3    South Australia     5223  Australia  Australia   9/17/1957  \n",
      "4           Victoria     3698  Australia  Australia  11/19/1965  \n",
      "\n",
      "========================================\n",
      "Analyzing Data_Dictionary.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Table          0\n",
      "Field          0\n",
      "Description    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Table          object\n",
      "Field          object\n",
      "Description    object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   Table          Field                                        Description\n",
      "0  Sales   Order Number                           Unique ID for each order\n",
      "1  Sales      Line Item  Identifies individual products purchased as pa...\n",
      "2  Sales     Order Date                          Date the order was placed\n",
      "3  Sales  Delivery Date                       Date the order was delivered\n",
      "4  Sales    CustomerKey  Unique key identifying which customer placed t...\n",
      "\n",
      "========================================\n",
      "Analyzing Data_Dictionary.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Table          0\n",
      "Field          0\n",
      "Description    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Table          object\n",
      "Field          object\n",
      "Description    object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   Table          Field                                        Description\n",
      "0  Sales   Order Number                           Unique ID for each order\n",
      "1  Sales      Line Item  Identifies individual products purchased as pa...\n",
      "2  Sales     Order Date                          Date the order was placed\n",
      "3  Sales  Delivery Date                       Date the order was delivered\n",
      "4  Sales    CustomerKey  Unique key identifying which customer placed t...\n",
      "\n",
      "========================================\n",
      "Analyzing Data_Dictionary.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Table          0\n",
      "Field          0\n",
      "Description    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Table          object\n",
      "Field          object\n",
      "Description    object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   Table          Field                                        Description\n",
      "0  Sales   Order Number                           Unique ID for each order\n",
      "1  Sales      Line Item  Identifies individual products purchased as pa...\n",
      "2  Sales     Order Date                          Date the order was placed\n",
      "3  Sales  Delivery Date                       Date the order was delivered\n",
      "4  Sales    CustomerKey  Unique key identifying which customer placed t...\n",
      "\n",
      "========================================\n",
      "Analyzing Data_Dictionary.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Table          0\n",
      "Field          0\n",
      "Description    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Table          object\n",
      "Field          object\n",
      "Description    object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   Table          Field                                        Description\n",
      "0  Sales   Order Number                           Unique ID for each order\n",
      "1  Sales      Line Item  Identifies individual products purchased as pa...\n",
      "2  Sales     Order Date                          Date the order was placed\n",
      "3  Sales  Delivery Date                       Date the order was delivered\n",
      "4  Sales    CustomerKey  Unique key identifying which customer placed t...\n",
      "\n",
      "========================================\n",
      "Analyzing Exchange_Rates.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Date        0\n",
      "Currency    0\n",
      "Exchange    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Date         object\n",
      "Currency     object\n",
      "Exchange    float64\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "       Date Currency  Exchange\n",
      "0  1/1/2015      USD    1.0000\n",
      "1  1/1/2015      CAD    1.1583\n",
      "2  1/1/2015      AUD    1.2214\n",
      "3  1/1/2015      EUR    0.8237\n",
      "4  1/1/2015      GBP    0.6415\n",
      "\n",
      "========================================\n",
      "Analyzing Exchange_Rates.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Date        0\n",
      "Currency    0\n",
      "Exchange    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Date         object\n",
      "Currency     object\n",
      "Exchange    float64\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "       Date Currency  Exchange\n",
      "0  1/1/2015      USD    1.0000\n",
      "1  1/1/2015      CAD    1.1583\n",
      "2  1/1/2015      AUD    1.2214\n",
      "3  1/1/2015      EUR    0.8237\n",
      "4  1/1/2015      GBP    0.6415\n",
      "\n",
      "========================================\n",
      "Analyzing Exchange_Rates.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Date        0\n",
      "Currency    0\n",
      "Exchange    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Date         object\n",
      "Currency     object\n",
      "Exchange    float64\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "       Date Currency  Exchange\n",
      "0  1/1/2015      USD    1.0000\n",
      "1  1/1/2015      CAD    1.1583\n",
      "2  1/1/2015      AUD    1.2214\n",
      "3  1/1/2015      EUR    0.8237\n",
      "4  1/1/2015      GBP    0.6415\n",
      "\n",
      "========================================\n",
      "Analyzing Exchange_Rates.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Date        0\n",
      "Currency    0\n",
      "Exchange    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Date         object\n",
      "Currency     object\n",
      "Exchange    float64\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "       Date Currency  Exchange\n",
      "0  1/1/2015      USD    1.0000\n",
      "1  1/1/2015      CAD    1.1583\n",
      "2  1/1/2015      AUD    1.2214\n",
      "3  1/1/2015      EUR    0.8237\n",
      "4  1/1/2015      GBP    0.6415\n",
      "\n",
      "========================================\n",
      "Analyzing Products.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "ProductKey        0\n",
      "Product Name      0\n",
      "Brand             0\n",
      "Color             0\n",
      "Unit Cost USD     0\n",
      "Unit Price USD    0\n",
      "SubcategoryKey    0\n",
      "Subcategory       0\n",
      "CategoryKey       0\n",
      "Category          0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "ProductKey         int64\n",
      "Product Name      object\n",
      "Brand             object\n",
      "Color             object\n",
      "Unit Cost USD     object\n",
      "Unit Price USD    object\n",
      "SubcategoryKey     int64\n",
      "Subcategory       object\n",
      "CategoryKey        int64\n",
      "Category          object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "  Unit Cost USD Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0        $6.62         $12.99              101     MP4&MP3            1   \n",
      "1        $6.62         $12.99              101     MP4&MP3            1   \n",
      "2        $7.40         $14.52              101     MP4&MP3            1   \n",
      "3       $11.00         $21.57              101     MP4&MP3            1   \n",
      "4       $11.00         $21.57              101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n",
      "\n",
      "========================================\n",
      "Analyzing Products.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "ProductKey        0\n",
      "Product Name      0\n",
      "Brand             0\n",
      "Color             0\n",
      "Unit Cost USD     0\n",
      "Unit Price USD    0\n",
      "SubcategoryKey    0\n",
      "Subcategory       0\n",
      "CategoryKey       0\n",
      "Category          0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "ProductKey         int64\n",
      "Product Name      object\n",
      "Brand             object\n",
      "Color             object\n",
      "Unit Cost USD     object\n",
      "Unit Price USD    object\n",
      "SubcategoryKey     int64\n",
      "Subcategory       object\n",
      "CategoryKey        int64\n",
      "Category          object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "  Unit Cost USD Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0        $6.62         $12.99              101     MP4&MP3            1   \n",
      "1        $6.62         $12.99              101     MP4&MP3            1   \n",
      "2        $7.40         $14.52              101     MP4&MP3            1   \n",
      "3       $11.00         $21.57              101     MP4&MP3            1   \n",
      "4       $11.00         $21.57              101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n",
      "\n",
      "========================================\n",
      "Analyzing Products.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "ProductKey        0\n",
      "Product Name      0\n",
      "Brand             0\n",
      "Color             0\n",
      "Unit Cost USD     0\n",
      "Unit Price USD    0\n",
      "SubcategoryKey    0\n",
      "Subcategory       0\n",
      "CategoryKey       0\n",
      "Category          0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "ProductKey         int64\n",
      "Product Name      object\n",
      "Brand             object\n",
      "Color             object\n",
      "Unit Cost USD     object\n",
      "Unit Price USD    object\n",
      "SubcategoryKey     int64\n",
      "Subcategory       object\n",
      "CategoryKey        int64\n",
      "Category          object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "  Unit Cost USD Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0        $6.62         $12.99              101     MP4&MP3            1   \n",
      "1        $6.62         $12.99              101     MP4&MP3            1   \n",
      "2        $7.40         $14.52              101     MP4&MP3            1   \n",
      "3       $11.00         $21.57              101     MP4&MP3            1   \n",
      "4       $11.00         $21.57              101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n",
      "\n",
      "========================================\n",
      "Analyzing Products.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "ProductKey        0\n",
      "Product Name      0\n",
      "Brand             0\n",
      "Color             0\n",
      "Unit Cost USD     0\n",
      "Unit Price USD    0\n",
      "SubcategoryKey    0\n",
      "Subcategory       0\n",
      "CategoryKey       0\n",
      "Category          0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "ProductKey         int64\n",
      "Product Name      object\n",
      "Brand             object\n",
      "Color             object\n",
      "Unit Cost USD     object\n",
      "Unit Price USD    object\n",
      "SubcategoryKey     int64\n",
      "Subcategory       object\n",
      "CategoryKey        int64\n",
      "Category          object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "  Unit Cost USD Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0        $6.62         $12.99              101     MP4&MP3            1   \n",
      "1        $6.62         $12.99              101     MP4&MP3            1   \n",
      "2        $7.40         $14.52              101     MP4&MP3            1   \n",
      "3       $11.00         $21.57              101     MP4&MP3            1   \n",
      "4       $11.00         $21.57              101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n",
      "\n",
      "========================================\n",
      "Analyzing Sales.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Order Number         0\n",
      "Line Item            0\n",
      "Order Date           0\n",
      "Delivery Date    49719\n",
      "CustomerKey          0\n",
      "StoreKey             0\n",
      "ProductKey           0\n",
      "Quantity             0\n",
      "Currency Code        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Order Number      int64\n",
      "Line Item         int64\n",
      "Order Date       object\n",
      "Delivery Date    object\n",
      "CustomerKey       int64\n",
      "StoreKey          int64\n",
      "ProductKey        int64\n",
      "Quantity          int64\n",
      "Currency Code    object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   Order Number  Line Item Order Date Delivery Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1   1/1/2016           NaN       265598        10   \n",
      "1        366001          1   1/1/2016     1/13/2016      1269051         0   \n",
      "2        366001          2   1/1/2016     1/13/2016      1269051         0   \n",
      "3        366002          1   1/1/2016     1/12/2016       266019         0   \n",
      "4        366002          2   1/1/2016     1/12/2016       266019         0   \n",
      "\n",
      "   ProductKey  Quantity Currency Code  \n",
      "0        1304         1           CAD  \n",
      "1        1048         2           USD  \n",
      "2        2007         1           USD  \n",
      "3        1106         7           CAD  \n",
      "4         373         1           CAD  \n",
      "\n",
      "========================================\n",
      "Analyzing Sales.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Order Number         0\n",
      "Line Item            0\n",
      "Order Date           0\n",
      "Delivery Date    49719\n",
      "CustomerKey          0\n",
      "StoreKey             0\n",
      "ProductKey           0\n",
      "Quantity             0\n",
      "Currency Code        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Order Number      int64\n",
      "Line Item         int64\n",
      "Order Date       object\n",
      "Delivery Date    object\n",
      "CustomerKey       int64\n",
      "StoreKey          int64\n",
      "ProductKey        int64\n",
      "Quantity          int64\n",
      "Currency Code    object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   Order Number  Line Item Order Date Delivery Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1   1/1/2016           NaN       265598        10   \n",
      "1        366001          1   1/1/2016     1/13/2016      1269051         0   \n",
      "2        366001          2   1/1/2016     1/13/2016      1269051         0   \n",
      "3        366002          1   1/1/2016     1/12/2016       266019         0   \n",
      "4        366002          2   1/1/2016     1/12/2016       266019         0   \n",
      "\n",
      "   ProductKey  Quantity Currency Code  \n",
      "0        1304         1           CAD  \n",
      "1        1048         2           USD  \n",
      "2        2007         1           USD  \n",
      "3        1106         7           CAD  \n",
      "4         373         1           CAD  \n",
      "\n",
      "========================================\n",
      "Analyzing Sales.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Order Number         0\n",
      "Line Item            0\n",
      "Order Date           0\n",
      "Delivery Date    49719\n",
      "CustomerKey          0\n",
      "StoreKey             0\n",
      "ProductKey           0\n",
      "Quantity             0\n",
      "Currency Code        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Order Number      int64\n",
      "Line Item         int64\n",
      "Order Date       object\n",
      "Delivery Date    object\n",
      "CustomerKey       int64\n",
      "StoreKey          int64\n",
      "ProductKey        int64\n",
      "Quantity          int64\n",
      "Currency Code    object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   Order Number  Line Item Order Date Delivery Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1   1/1/2016           NaN       265598        10   \n",
      "1        366001          1   1/1/2016     1/13/2016      1269051         0   \n",
      "2        366001          2   1/1/2016     1/13/2016      1269051         0   \n",
      "3        366002          1   1/1/2016     1/12/2016       266019         0   \n",
      "4        366002          2   1/1/2016     1/12/2016       266019         0   \n",
      "\n",
      "   ProductKey  Quantity Currency Code  \n",
      "0        1304         1           CAD  \n",
      "1        1048         2           USD  \n",
      "2        2007         1           USD  \n",
      "3        1106         7           CAD  \n",
      "4         373         1           CAD  \n",
      "\n",
      "========================================\n",
      "Analyzing Sales.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "Order Number         0\n",
      "Line Item            0\n",
      "Order Date           0\n",
      "Delivery Date    49719\n",
      "CustomerKey          0\n",
      "StoreKey             0\n",
      "ProductKey           0\n",
      "Quantity             0\n",
      "Currency Code        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "Order Number      int64\n",
      "Line Item         int64\n",
      "Order Date       object\n",
      "Delivery Date    object\n",
      "CustomerKey       int64\n",
      "StoreKey          int64\n",
      "ProductKey        int64\n",
      "Quantity          int64\n",
      "Currency Code    object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   Order Number  Line Item Order Date Delivery Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1   1/1/2016           NaN       265598        10   \n",
      "1        366001          1   1/1/2016     1/13/2016      1269051         0   \n",
      "2        366001          2   1/1/2016     1/13/2016      1269051         0   \n",
      "3        366002          1   1/1/2016     1/12/2016       266019         0   \n",
      "4        366002          2   1/1/2016     1/12/2016       266019         0   \n",
      "\n",
      "   ProductKey  Quantity Currency Code  \n",
      "0        1304         1           CAD  \n",
      "1        1048         2           USD  \n",
      "2        2007         1           USD  \n",
      "3        1106         7           CAD  \n",
      "4         373         1           CAD  \n",
      "\n",
      "========================================\n",
      "Analyzing Stores.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "StoreKey         0\n",
      "Country          0\n",
      "State            0\n",
      "Square Meters    1\n",
      "Open Date        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "StoreKey           int64\n",
      "Country           object\n",
      "State             object\n",
      "Square Meters    float64\n",
      "Open Date         object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0   1/1/2008\n",
      "1         2  Australia            Northern Territory          665.0  1/12/2008\n",
      "2         3  Australia               South Australia         2000.0   1/7/2012\n",
      "3         4  Australia                      Tasmania         2000.0   1/1/2010\n",
      "4         5  Australia                      Victoria         2000.0  12/9/2015\n",
      "\n",
      "========================================\n",
      "Analyzing Stores.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "StoreKey         0\n",
      "Country          0\n",
      "State            0\n",
      "Square Meters    1\n",
      "Open Date        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "StoreKey           int64\n",
      "Country           object\n",
      "State             object\n",
      "Square Meters    float64\n",
      "Open Date         object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0   1/1/2008\n",
      "1         2  Australia            Northern Territory          665.0  1/12/2008\n",
      "2         3  Australia               South Australia         2000.0   1/7/2012\n",
      "3         4  Australia                      Tasmania         2000.0   1/1/2010\n",
      "4         5  Australia                      Victoria         2000.0  12/9/2015\n",
      "\n",
      "========================================\n",
      "Analyzing Stores.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "StoreKey         0\n",
      "Country          0\n",
      "State            0\n",
      "Square Meters    1\n",
      "Open Date        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "StoreKey           int64\n",
      "Country           object\n",
      "State             object\n",
      "Square Meters    float64\n",
      "Open Date         object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0   1/1/2008\n",
      "1         2  Australia            Northern Territory          665.0  1/12/2008\n",
      "2         3  Australia               South Australia         2000.0   1/7/2012\n",
      "3         4  Australia                      Tasmania         2000.0   1/1/2010\n",
      "4         5  Australia                      Victoria         2000.0  12/9/2015\n",
      "\n",
      "========================================\n",
      "Analyzing Stores.csv\n",
      "========================================\n",
      "\n",
      "Missing Values:\n",
      "StoreKey         0\n",
      "Country          0\n",
      "State            0\n",
      "Square Meters    1\n",
      "Open Date        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows:\n",
      "0\n",
      "\n",
      "Data Types:\n",
      "StoreKey           int64\n",
      "Country           object\n",
      "State             object\n",
      "Square Meters    float64\n",
      "Open Date         object\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0   1/1/2008\n",
      "1         2  Australia            Northern Territory          665.0  1/12/2008\n",
      "2         3  Australia               South Australia         2000.0   1/7/2012\n",
      "3         4  Australia                      Tasmania         2000.0   1/1/2010\n",
      "4         5  Australia                      Victoria         2000.0  12/9/2015\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_csv_file(file_path, encoding='utf-8'):\n",
    "    try:\n",
    "        # Load the CSV file with the specified encoding\n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        \n",
    "        # Print basic info\n",
    "        print(f\"\\n{'='*40}\\nAnalyzing {os.path.basename(file_path)}\\n{'='*40}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "        # Check for duplicate rows\n",
    "        print(\"\\nDuplicate Rows:\")\n",
    "        print(df.duplicated().sum())\n",
    "        \n",
    "        # Check the data types of each column\n",
    "        print(\"\\nData Types:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        # Display the first few rows of the dataframe\n",
    "        print(\"\\nFirst Few Rows:\")\n",
    "        print(df.head())\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading {file_path} with {encoding} encoding: {e}\")\n",
    "\n",
    "# Paths for each file\n",
    "file_paths = [\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Customers.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Data_Dictionary.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Products.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Sales.csv\",\n",
    "    \"/Users/shadivaz/Desktop/PROJECT 2/Stores.csv\"\n",
    "]\n",
    "\n",
    "# List of encodings to try\n",
    "encodings_to_try = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "# Check each file individually with different encodings\n",
    "for path in file_paths:\n",
    "    for encoding in encodings_to_try:\n",
    "        check_csv_file(path, encoding)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b88707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "with open('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read(10000))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81ab7cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfc in position 5460: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/shadivaz/Desktop/PROJECT 2/Customers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Find missing data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m missing_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xfc in position 5460: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv')\n",
    "\n",
    "# Find missing data\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Print the results\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f87c71da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerKey</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State Code</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Birthday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lilly Harding</td>\n",
       "      <td>WANDEARAH EAST</td>\n",
       "      <td>SA</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>5523</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>7/3/1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325</td>\n",
       "      <td>Female</td>\n",
       "      <td>Madison Hull</td>\n",
       "      <td>MOUNT BUDD</td>\n",
       "      <td>WA</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>6522</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>9/27/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>554</td>\n",
       "      <td>Female</td>\n",
       "      <td>Claire Ferres</td>\n",
       "      <td>WINJALLOK</td>\n",
       "      <td>VIC</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3380</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>5/26/1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jai Poltpalingada</td>\n",
       "      <td>MIDDLE RIVER</td>\n",
       "      <td>SA</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>5223</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>9/17/1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1042</td>\n",
       "      <td>Male</td>\n",
       "      <td>Aidan Pankhurst</td>\n",
       "      <td>TAWONGA SOUTH</td>\n",
       "      <td>VIC</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3698</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>11/19/1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15261</th>\n",
       "      <td>2099600</td>\n",
       "      <td>Female</td>\n",
       "      <td>Denisa Duková</td>\n",
       "      <td>Houston</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77017</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>3/25/1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15262</th>\n",
       "      <td>2099618</td>\n",
       "      <td>Male</td>\n",
       "      <td>Justin Solórzano</td>\n",
       "      <td>Mclean</td>\n",
       "      <td>VA</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>22101</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>2/16/1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15263</th>\n",
       "      <td>2099758</td>\n",
       "      <td>Male</td>\n",
       "      <td>Svend Petrussen</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>NC</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28405</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>11/9/1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15264</th>\n",
       "      <td>2099862</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lorenza Rush</td>\n",
       "      <td>Riverside</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>92501</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>10/12/1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15265</th>\n",
       "      <td>2099937</td>\n",
       "      <td>Male</td>\n",
       "      <td>Zygmunt Kaminski</td>\n",
       "      <td>Bloomfield Township</td>\n",
       "      <td>MI</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>48302</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>8/18/1965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15266 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustomerKey  Gender               Name                 City State Code  \\\n",
       "0              301  Female      Lilly Harding       WANDEARAH EAST         SA   \n",
       "1              325  Female       Madison Hull           MOUNT BUDD         WA   \n",
       "2              554  Female      Claire Ferres            WINJALLOK        VIC   \n",
       "3              786    Male  Jai Poltpalingada         MIDDLE RIVER         SA   \n",
       "4             1042    Male    Aidan Pankhurst        TAWONGA SOUTH        VIC   \n",
       "...            ...     ...                ...                  ...        ...   \n",
       "15261      2099600  Female     Denisa Duková              Houston         TX   \n",
       "15262      2099618    Male   Justin Solórzano               Mclean         VA   \n",
       "15263      2099758    Male    Svend Petrussen           Wilmington         NC   \n",
       "15264      2099862  Female       Lorenza Rush            Riverside         CA   \n",
       "15265      2099937    Male   Zygmunt Kaminski  Bloomfield Township         MI   \n",
       "\n",
       "                   State Zip Code        Country      Continent    Birthday  \n",
       "0        South Australia     5523      Australia      Australia    7/3/1939  \n",
       "1      Western Australia     6522      Australia      Australia   9/27/1979  \n",
       "2               Victoria     3380      Australia      Australia   5/26/1947  \n",
       "3        South Australia     5223      Australia      Australia   9/17/1957  \n",
       "4               Victoria     3698      Australia      Australia  11/19/1965  \n",
       "...                  ...      ...            ...            ...         ...  \n",
       "15261              Texas    77017  United States  North America   3/25/1936  \n",
       "15262           Virginia    22101  United States  North America   2/16/1992  \n",
       "15263     North Carolina    28405  United States  North America   11/9/1937  \n",
       "15264         California    92501  United States  North America  10/12/1937  \n",
       "15265           Michigan    48302  United States  North America   8/18/1965  \n",
       "\n",
       "[15266 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding='latin1')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dce35c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerKey     0\n",
      "Gender          0\n",
      "Name            0\n",
      "City            0\n",
      "State Code     10\n",
      "State           0\n",
      "Zip Code        0\n",
      "Country         0\n",
      "Continent       0\n",
      "Birthday        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b8560a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b53402cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.isnull().sum().plot(kind='bar')\n",
    "plt.title('Missing Values in Each Column')\n",
    "plt.xlabel('Column Name')\n",
    "plt.ylabel('Count of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44ce039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['State Code']\n"
     ]
    }
   ],
   "source": [
    "missing_cols = [col for col in df.columns if df[col].isnull().any()]\n",
    "print(missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9612b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CustomerKey  Gender                Name               City State Code  \\\n",
      "5304       729681  Female    Rossana Padovesi            Polvica        NaN   \n",
      "5316       732289    Male        Indro Piccio          Varcaturo        NaN   \n",
      "5372       742042    Male     Amaranto Loggia          Casaferro        NaN   \n",
      "5377       742886  Female       Edmonda Capon           Terzigno        NaN   \n",
      "5378       743343  Female        Ambra Sagese  Pomigliano D'Arco        NaN   \n",
      "5485       759705    Male    Callisto Lo Duca            Casilli        NaN   \n",
      "5525       765589    Male  Michelino Lucchesi       Pompei Scavi        NaN   \n",
      "5531       766410    Male   Adelmio Beneventi             Licola        NaN   \n",
      "5631       781667  Female          Ilda Manna             Napoli        NaN   \n",
      "5695       789177    Male   Calogero Folliero      Mariglianella        NaN   \n",
      "\n",
      "       State Zip Code Country Continent    Birthday  \n",
      "5304  Napoli    80035   Italy    Europe   4/18/1981  \n",
      "5316  Napoli    80014   Italy    Europe   2/24/1949  \n",
      "5372  Napoli    80034   Italy    Europe   3/14/1936  \n",
      "5377  Napoli    80040   Italy    Europe    8/6/1963  \n",
      "5378  Napoli    80038   Italy    Europe    1/5/1961  \n",
      "5485  Napoli    80047   Italy    Europe   8/28/1976  \n",
      "5525  Napoli    80045   Italy    Europe  11/13/1947  \n",
      "5531  Napoli    80078   Italy    Europe   1/13/1940  \n",
      "5631  Napoli    80134   Italy    Europe    5/8/1977  \n",
      "5695  Napoli    80030   Italy    Europe    3/3/2000  \n"
     ]
    }
   ],
   "source": [
    "missing_rows = df[df.isnull().any(axis=1)]\n",
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e06b3de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CustomerKey', 'Gender', 'Name', 'City', 'State Code', 'State',\n",
      "       'Zip Code', 'Country', 'Continent', 'Birthday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)  # Check the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa327264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_16198/829835071.py:11: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sales_data['Delivery Date'].fillna(method='ffill', inplace=True)\n",
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_16198/829835071.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sales_data['Delivery Date'].fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Number</th>\n",
       "      <th>Line Item</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Delivery Date</th>\n",
       "      <th>CustomerKey</th>\n",
       "      <th>StoreKey</th>\n",
       "      <th>ProductKey</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Currency Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>366000</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>265598</td>\n",
       "      <td>10</td>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366001</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>1269051</td>\n",
       "      <td>0</td>\n",
       "      <td>1048</td>\n",
       "      <td>2</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>366001</td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>1269051</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366002</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>266019</td>\n",
       "      <td>0</td>\n",
       "      <td>1106</td>\n",
       "      <td>7</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>366002</td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>266019</td>\n",
       "      <td>0</td>\n",
       "      <td>373</td>\n",
       "      <td>1</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62879</th>\n",
       "      <td>2243030</td>\n",
       "      <td>1</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-27</td>\n",
       "      <td>1216913</td>\n",
       "      <td>43</td>\n",
       "      <td>632</td>\n",
       "      <td>3</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62880</th>\n",
       "      <td>2243031</td>\n",
       "      <td>1</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>511229</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62881</th>\n",
       "      <td>2243032</td>\n",
       "      <td>1</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>331277</td>\n",
       "      <td>0</td>\n",
       "      <td>1613</td>\n",
       "      <td>2</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62882</th>\n",
       "      <td>2243032</td>\n",
       "      <td>2</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>331277</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>2</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62883</th>\n",
       "      <td>2243032</td>\n",
       "      <td>3</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>331277</td>\n",
       "      <td>0</td>\n",
       "      <td>464</td>\n",
       "      <td>7</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62884 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order Number  Line Item Order Date Delivery Date  CustomerKey  \\\n",
       "0            366000          1   1/1/2016    2016-01-13       265598   \n",
       "1            366001          1   1/1/2016    2016-01-13      1269051   \n",
       "2            366001          2   1/1/2016    2016-01-13      1269051   \n",
       "3            366002          1   1/1/2016    2016-01-12       266019   \n",
       "4            366002          2   1/1/2016    2016-01-12       266019   \n",
       "...             ...        ...        ...           ...          ...   \n",
       "62879       2243030          1  2/20/2021    2021-02-27      1216913   \n",
       "62880       2243031          1  2/20/2021    2021-02-24       511229   \n",
       "62881       2243032          1  2/20/2021    2021-02-23       331277   \n",
       "62882       2243032          2  2/20/2021    2021-02-23       331277   \n",
       "62883       2243032          3  2/20/2021    2021-02-23       331277   \n",
       "\n",
       "       StoreKey  ProductKey  Quantity Currency Code  \n",
       "0            10        1304         1           CAD  \n",
       "1             0        1048         2           USD  \n",
       "2             0        2007         1           USD  \n",
       "3             0        1106         7           CAD  \n",
       "4             0         373         1           CAD  \n",
       "...         ...         ...       ...           ...  \n",
       "62879        43         632         3           USD  \n",
       "62880         0          98         4           EUR  \n",
       "62881         0        1613         2           CAD  \n",
       "62882         0        1717         2           CAD  \n",
       "62883         0         464         7           CAD  \n",
       "\n",
       "[62884 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path =  \"/Users/shadivaz/Desktop/PROJECT 2/Sales.csv\"\n",
    "sales_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Delivery Date to datetime if not already done\n",
    "sales_data['Delivery Date'] = pd.to_datetime(sales_data['Delivery Date'])\n",
    "\n",
    "# Forward fill the missing values\n",
    "sales_data['Delivery Date'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# If you also want to backward fill the remaining missing values (if any)\n",
    "sales_data['Delivery Date'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Display the DataFrame to check the result\n",
    "sales_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "071dbce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_16198/3958157199.py:11: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sales_data['Delivery Date'].fillna(method='ffill', inplace=True)\n",
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_16198/3958157199.py:14: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sales_data['Delivery Date'].fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to /Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Users/shadivaz/Desktop/PROJECT 2/Sales.csv'\n",
    "sales_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Delivery Date to datetime if not already done\n",
    "sales_data['Delivery Date'] = pd.to_datetime(sales_data['Delivery Date'])\n",
    "\n",
    "# Forward fill the missing values\n",
    "sales_data['Delivery Date'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Backward fill the remaining missing values (if any)\n",
    "sales_data['Delivery Date'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Save the DataFrame back to a CSV file\n",
    "output_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv'\n",
    "sales_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18fc84",
   "metadata": {},
   "source": [
    "df= pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ed3facb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Number</th>\n",
       "      <th>Line Item</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Delivery Date</th>\n",
       "      <th>CustomerKey</th>\n",
       "      <th>StoreKey</th>\n",
       "      <th>ProductKey</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Currency Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>366000</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>265598</td>\n",
       "      <td>10</td>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366001</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>1269051</td>\n",
       "      <td>0</td>\n",
       "      <td>1048</td>\n",
       "      <td>2</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>366001</td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>1269051</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366002</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>266019</td>\n",
       "      <td>0</td>\n",
       "      <td>1106</td>\n",
       "      <td>7</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>366002</td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/2016</td>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>266019</td>\n",
       "      <td>0</td>\n",
       "      <td>373</td>\n",
       "      <td>1</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62879</th>\n",
       "      <td>2243030</td>\n",
       "      <td>1</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-27</td>\n",
       "      <td>1216913</td>\n",
       "      <td>43</td>\n",
       "      <td>632</td>\n",
       "      <td>3</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62880</th>\n",
       "      <td>2243031</td>\n",
       "      <td>1</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>511229</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62881</th>\n",
       "      <td>2243032</td>\n",
       "      <td>1</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>331277</td>\n",
       "      <td>0</td>\n",
       "      <td>1613</td>\n",
       "      <td>2</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62882</th>\n",
       "      <td>2243032</td>\n",
       "      <td>2</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>331277</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>2</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62883</th>\n",
       "      <td>2243032</td>\n",
       "      <td>3</td>\n",
       "      <td>2/20/2021</td>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>331277</td>\n",
       "      <td>0</td>\n",
       "      <td>464</td>\n",
       "      <td>7</td>\n",
       "      <td>CAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62884 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order Number  Line Item Order Date Delivery Date  CustomerKey  \\\n",
       "0            366000          1   1/1/2016    2016-01-13       265598   \n",
       "1            366001          1   1/1/2016    2016-01-13      1269051   \n",
       "2            366001          2   1/1/2016    2016-01-13      1269051   \n",
       "3            366002          1   1/1/2016    2016-01-12       266019   \n",
       "4            366002          2   1/1/2016    2016-01-12       266019   \n",
       "...             ...        ...        ...           ...          ...   \n",
       "62879       2243030          1  2/20/2021    2021-02-27      1216913   \n",
       "62880       2243031          1  2/20/2021    2021-02-24       511229   \n",
       "62881       2243032          1  2/20/2021    2021-02-23       331277   \n",
       "62882       2243032          2  2/20/2021    2021-02-23       331277   \n",
       "62883       2243032          3  2/20/2021    2021-02-23       331277   \n",
       "\n",
       "       StoreKey  ProductKey  Quantity Currency Code  \n",
       "0            10        1304         1           CAD  \n",
       "1             0        1048         2           USD  \n",
       "2             0        2007         1           USD  \n",
       "3             0        1106         7           CAD  \n",
       "4             0         373         1           CAD  \n",
       "...         ...         ...       ...           ...  \n",
       "62879        43         632         3           USD  \n",
       "62880         0          98         4           EUR  \n",
       "62881         0        1613         2           CAD  \n",
       "62882         0        1717         2           CAD  \n",
       "62883         0         464         7           CAD  \n",
       "\n",
       "[62884 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8e1df6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62884 entries, 0 to 62883\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Order Number   62884 non-null  int64 \n",
      " 1   Line Item      62884 non-null  int64 \n",
      " 2   Order Date     62884 non-null  object\n",
      " 3   Delivery Date  62884 non-null  object\n",
      " 4   CustomerKey    62884 non-null  int64 \n",
      " 5   StoreKey       62884 non-null  int64 \n",
      " 6   ProductKey     62884 non-null  int64 \n",
      " 7   Quantity       62884 non-null  int64 \n",
      " 8   Currency Code  62884 non-null  object\n",
      "dtypes: int64(6), object(3)\n",
      "memory usage: 4.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5a2c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Exchange_Rates.csv file\n",
    "exchange_rates_path = '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv'\n",
    "\n",
    "# Load the exchange rates data\n",
    "exchange_rates_df = pd.read_csv(exchange_rates_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d961beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date Currency  Exchange\n",
      "0  1/1/2015      USD    1.0000\n",
      "1  1/1/2015      CAD    1.1583\n",
      "2  1/1/2015      AUD    1.2214\n",
      "3  1/1/2015      EUR    0.8237\n",
      "4  1/1/2015      GBP    0.6415\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11215 entries, 0 to 11214\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Date      11215 non-null  object \n",
      " 1   Currency  11215 non-null  object \n",
      " 2   Exchange  11215 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 263.0+ KB\n",
      "None\n",
      "           Exchange\n",
      "count  11215.000000\n",
      "mean       1.061682\n",
      "std        0.245519\n",
      "min        0.628500\n",
      "25%        0.857800\n",
      "50%        1.000000\n",
      "75%        1.311900\n",
      "max        1.725300\n"
     ]
    }
   ],
   "source": [
    "# View the first few rows of the dataset\n",
    "print(exchange_rates_df.head())\n",
    "\n",
    "# Get an overview of the data types and missing values\n",
    "print(exchange_rates_df.info())\n",
    "\n",
    "# Summary statistics of the dataset\n",
    "print(exchange_rates_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72529f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date        0\n",
      "Currency    0\n",
      "Exchange    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "print(exchange_rates_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccf4ae97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date, Currency, Exchange]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for inconsistent currency codes\n",
    "invalid_currency_codes = exchange_rates_df[~exchange_rates_df['Currency'].str.match('^[A-Z]{3}$')]\n",
    "print(invalid_currency_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "640a1fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductKey</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Color</th>\n",
       "      <th>Unit Cost USD</th>\n",
       "      <th>Unit Price USD</th>\n",
       "      <th>SubcategoryKey</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>CategoryKey</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Contoso 512MB MP3 Player E51 Silver</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>Silver</td>\n",
       "      <td>$6.62</td>\n",
       "      <td>$12.99</td>\n",
       "      <td>101</td>\n",
       "      <td>MP4&amp;MP3</td>\n",
       "      <td>1</td>\n",
       "      <td>Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Contoso 512MB MP3 Player E51 Blue</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>Blue</td>\n",
       "      <td>$6.62</td>\n",
       "      <td>$12.99</td>\n",
       "      <td>101</td>\n",
       "      <td>MP4&amp;MP3</td>\n",
       "      <td>1</td>\n",
       "      <td>Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Contoso 1G MP3 Player E100 White</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>White</td>\n",
       "      <td>$7.40</td>\n",
       "      <td>$14.52</td>\n",
       "      <td>101</td>\n",
       "      <td>MP4&amp;MP3</td>\n",
       "      <td>1</td>\n",
       "      <td>Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Contoso 2G MP3 Player E200 Silver</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>Silver</td>\n",
       "      <td>$11.00</td>\n",
       "      <td>$21.57</td>\n",
       "      <td>101</td>\n",
       "      <td>MP4&amp;MP3</td>\n",
       "      <td>1</td>\n",
       "      <td>Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Contoso 2G MP3 Player E200 Red</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>Red</td>\n",
       "      <td>$11.00</td>\n",
       "      <td>$21.57</td>\n",
       "      <td>101</td>\n",
       "      <td>MP4&amp;MP3</td>\n",
       "      <td>1</td>\n",
       "      <td>Audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>2513</td>\n",
       "      <td>Contoso Bluetooth Active Headphones L15 Red</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>Red</td>\n",
       "      <td>$43.07</td>\n",
       "      <td>$129.99</td>\n",
       "      <td>505</td>\n",
       "      <td>Cell phones Accessories</td>\n",
       "      <td>5</td>\n",
       "      <td>Cell phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2514</td>\n",
       "      <td>Contoso Bluetooth Active Headphones L15 White</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>White</td>\n",
       "      <td>$43.07</td>\n",
       "      <td>$129.99</td>\n",
       "      <td>505</td>\n",
       "      <td>Cell phones Accessories</td>\n",
       "      <td>5</td>\n",
       "      <td>Cell phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2515</td>\n",
       "      <td>Contoso In-Line Coupler E180 White</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>White</td>\n",
       "      <td>$1.71</td>\n",
       "      <td>$3.35</td>\n",
       "      <td>505</td>\n",
       "      <td>Cell phones Accessories</td>\n",
       "      <td>5</td>\n",
       "      <td>Cell phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>2516</td>\n",
       "      <td>Contoso In-Line Coupler E180 Black</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>Black</td>\n",
       "      <td>$1.71</td>\n",
       "      <td>$3.35</td>\n",
       "      <td>505</td>\n",
       "      <td>Cell phones Accessories</td>\n",
       "      <td>5</td>\n",
       "      <td>Cell phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>2517</td>\n",
       "      <td>Contoso In-Line Coupler E180 Silver</td>\n",
       "      <td>Contoso</td>\n",
       "      <td>Silver</td>\n",
       "      <td>$1.71</td>\n",
       "      <td>$3.35</td>\n",
       "      <td>505</td>\n",
       "      <td>Cell phones Accessories</td>\n",
       "      <td>5</td>\n",
       "      <td>Cell phones</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ProductKey                                   Product Name    Brand  \\\n",
       "0              1            Contoso 512MB MP3 Player E51 Silver  Contoso   \n",
       "1              2              Contoso 512MB MP3 Player E51 Blue  Contoso   \n",
       "2              3               Contoso 1G MP3 Player E100 White  Contoso   \n",
       "3              4              Contoso 2G MP3 Player E200 Silver  Contoso   \n",
       "4              5                 Contoso 2G MP3 Player E200 Red  Contoso   \n",
       "...          ...                                            ...      ...   \n",
       "2512        2513    Contoso Bluetooth Active Headphones L15 Red  Contoso   \n",
       "2513        2514  Contoso Bluetooth Active Headphones L15 White  Contoso   \n",
       "2514        2515             Contoso In-Line Coupler E180 White  Contoso   \n",
       "2515        2516             Contoso In-Line Coupler E180 Black  Contoso   \n",
       "2516        2517            Contoso In-Line Coupler E180 Silver  Contoso   \n",
       "\n",
       "       Color Unit Cost USD Unit Price USD  SubcategoryKey  \\\n",
       "0     Silver        $6.62         $12.99              101   \n",
       "1       Blue        $6.62         $12.99              101   \n",
       "2      White        $7.40         $14.52              101   \n",
       "3     Silver       $11.00         $21.57              101   \n",
       "4        Red       $11.00         $21.57              101   \n",
       "...      ...           ...            ...             ...   \n",
       "2512     Red       $43.07        $129.99              505   \n",
       "2513   White       $43.07        $129.99              505   \n",
       "2514   White        $1.71          $3.35              505   \n",
       "2515   Black        $1.71          $3.35              505   \n",
       "2516  Silver        $1.71          $3.35              505   \n",
       "\n",
       "                  Subcategory  CategoryKey     Category  \n",
       "0                     MP4&MP3            1        Audio  \n",
       "1                     MP4&MP3            1        Audio  \n",
       "2                     MP4&MP3            1        Audio  \n",
       "3                     MP4&MP3            1        Audio  \n",
       "4                     MP4&MP3            1        Audio  \n",
       "...                       ...          ...          ...  \n",
       "2512  Cell phones Accessories            5  Cell phones  \n",
       "2513  Cell phones Accessories            5  Cell phones  \n",
       "2514  Cell phones Accessories            5  Cell phones  \n",
       "2515  Cell phones Accessories            5  Cell phones  \n",
       "2516  Cell phones Accessories            5  Cell phones  \n",
       "\n",
       "[2517 rows x 10 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Products.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "459b38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2517 entries, 0 to 2516\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ProductKey      2517 non-null   int64 \n",
      " 1   Product Name    2517 non-null   object\n",
      " 2   Brand           2517 non-null   object\n",
      " 3   Color           2517 non-null   object\n",
      " 4   Unit Cost USD   2517 non-null   object\n",
      " 5   Unit Price USD  2517 non-null   object\n",
      " 6   SubcategoryKey  2517 non-null   int64 \n",
      " 7   Subcategory     2517 non-null   object\n",
      " 8   CategoryKey     2517 non-null   int64 \n",
      " 9   Category        2517 non-null   object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 196.8+ KB\n",
      "None\n",
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "  Unit Cost USD Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0        $6.62         $12.99              101     MP4&MP3            1   \n",
      "1        $6.62         $12.99              101     MP4&MP3            1   \n",
      "2        $7.40         $14.52              101     MP4&MP3            1   \n",
      "3       $11.00         $21.57              101     MP4&MP3            1   \n",
      "4       $11.00         $21.57              101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c64ab15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreKey</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Square Meters</th>\n",
       "      <th>Open Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australian Capital Territory</td>\n",
       "      <td>595.0</td>\n",
       "      <td>1/1/2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Northern Territory</td>\n",
       "      <td>665.0</td>\n",
       "      <td>1/12/2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1/7/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Tasmania</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1/1/2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12/9/2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>United States</td>\n",
       "      <td>Utah</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3/6/2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>United States</td>\n",
       "      <td>Washington DC</td>\n",
       "      <td>1330.0</td>\n",
       "      <td>1/1/2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>United States</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>1/1/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>840.0</td>\n",
       "      <td>1/1/2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>Online</td>\n",
       "      <td>Online</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    StoreKey        Country                         State  Square Meters  \\\n",
       "0          1      Australia  Australian Capital Territory          595.0   \n",
       "1          2      Australia            Northern Territory          665.0   \n",
       "2          3      Australia               South Australia         2000.0   \n",
       "3          4      Australia                      Tasmania         2000.0   \n",
       "4          5      Australia                      Victoria         2000.0   \n",
       "..       ...            ...                           ...            ...   \n",
       "62        63  United States                          Utah         2000.0   \n",
       "63        64  United States                 Washington DC         1330.0   \n",
       "64        65  United States                 West Virginia         1785.0   \n",
       "65        66  United States                       Wyoming          840.0   \n",
       "66         0         Online                        Online            NaN   \n",
       "\n",
       "    Open Date  \n",
       "0    1/1/2008  \n",
       "1   1/12/2008  \n",
       "2    1/7/2012  \n",
       "3    1/1/2010  \n",
       "4   12/9/2015  \n",
       "..        ...  \n",
       "62   3/6/2008  \n",
       "63   1/1/2010  \n",
       "64   1/1/2012  \n",
       "65   1/1/2014  \n",
       "66   1/1/2010  \n",
       "\n",
       "[67 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Stores.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c07ee67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67 entries, 0 to 66\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   StoreKey       67 non-null     int64  \n",
      " 1   Country        67 non-null     object \n",
      " 2   State          67 non-null     object \n",
      " 3   Square Meters  66 non-null     float64\n",
      " 4   Open Date      67 non-null     object \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 2.7+ KB\n",
      "None\n",
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0   1/1/2008\n",
      "1         2  Australia            Northern Territory          665.0  1/12/2008\n",
      "2         3  Australia               South Australia         2000.0   1/7/2012\n",
      "3         4  Australia                      Tasmania         2000.0   1/1/2010\n",
      "4         5  Australia                      Victoria         2000.0  12/9/2015\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1381f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Country\n",
      "United States     24\n",
      "Germany            9\n",
      "United Kingdom     7\n",
      "France             7\n",
      "Australia          6\n",
      "Netherlands        5\n",
      "Canada             5\n",
      "Italy              3\n",
      "Online             1\n",
      "Name: count, dtype: int64\n",
      "State\n",
      "Online                          1\n",
      "Australian Capital Territory    1\n",
      "Northern Territory              1\n",
      "Mississippi                     1\n",
      "Montana                         1\n",
      "                               ..\n",
      "New Brunswick                   1\n",
      "Newfoundland and Labrador       1\n",
      "Northwest Territories           1\n",
      "Nunavut                         1\n",
      "Yukon                           1\n",
      "Name: count, Length: 67, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stores_df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Stores.csv')\n",
    "\n",
    "# Handle missing value in Square Meters\n",
    "stores_df['Square Meters'].fillna(stores_df['Square Meters'].mean(), inplace=True)\n",
    "\n",
    "# Convert Open Date to datetime\n",
    "stores_df['Open Date'] = pd.to_datetime(stores_df['Open Date'])\n",
    "\n",
    "# Check for duplicates\n",
    "print(stores_df.duplicated().sum())\n",
    "\n",
    "# Check for invalid values\n",
    "print(stores_df['Country'].value_counts())\n",
    "print(stores_df['State'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d63fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62884 entries, 0 to 62883\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Order Number   62884 non-null  int64 \n",
      " 1   Line Item      62884 non-null  int64 \n",
      " 2   Order Date     62884 non-null  object\n",
      " 3   Delivery Date  62884 non-null  object\n",
      " 4   CustomerKey    62884 non-null  int64 \n",
      " 5   StoreKey       62884 non-null  int64 \n",
      " 6   ProductKey     62884 non-null  int64 \n",
      " 7   Quantity       62884 non-null  int64 \n",
      " 8   Currency Code  62884 non-null  object\n",
      "dtypes: int64(6), object(3)\n",
      "memory usage: 4.3+ MB\n",
      "None\n",
      "   Order Number  Line Item Order Date Delivery Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1   1/1/2016    2016-01-13       265598        10   \n",
      "1        366001          1   1/1/2016    2016-01-13      1269051         0   \n",
      "2        366001          2   1/1/2016    2016-01-13      1269051         0   \n",
      "3        366002          1   1/1/2016    2016-01-12       266019         0   \n",
      "4        366002          2   1/1/2016    2016-01-12       266019         0   \n",
      "\n",
      "   ProductKey  Quantity Currency Code  \n",
      "0        1304         1           CAD  \n",
      "1        1048         2           USD  \n",
      "2        2007         1           USD  \n",
      "3        1106         7           CAD  \n",
      "4         373         1           CAD  \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv')\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "058b1201",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfc in position 5460: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/shadivaz/Desktop/PROJECT 2/Customers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39minfo())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xfc in position 5460: invalid start byte"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv')\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37312e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "430b048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "\n",
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7869902b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerKey</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State Code</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Birthday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lilly Harding</td>\n",
       "      <td>WANDEARAH EAST</td>\n",
       "      <td>SA</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>5523</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>7/3/1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325</td>\n",
       "      <td>Female</td>\n",
       "      <td>Madison Hull</td>\n",
       "      <td>MOUNT BUDD</td>\n",
       "      <td>WA</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>6522</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>9/27/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>554</td>\n",
       "      <td>Female</td>\n",
       "      <td>Claire Ferres</td>\n",
       "      <td>WINJALLOK</td>\n",
       "      <td>VIC</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3380</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>5/26/1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jai Poltpalingada</td>\n",
       "      <td>MIDDLE RIVER</td>\n",
       "      <td>SA</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>5223</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>9/17/1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1042</td>\n",
       "      <td>Male</td>\n",
       "      <td>Aidan Pankhurst</td>\n",
       "      <td>TAWONGA SOUTH</td>\n",
       "      <td>VIC</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3698</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>11/19/1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15261</th>\n",
       "      <td>2099600</td>\n",
       "      <td>Female</td>\n",
       "      <td>Denisa Dušková</td>\n",
       "      <td>Houston</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77017</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>3/25/1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15262</th>\n",
       "      <td>2099618</td>\n",
       "      <td>Male</td>\n",
       "      <td>Justin Solórzano</td>\n",
       "      <td>Mclean</td>\n",
       "      <td>VA</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>22101</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>2/16/1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15263</th>\n",
       "      <td>2099758</td>\n",
       "      <td>Male</td>\n",
       "      <td>Svend Petrussen</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>NC</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28405</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>11/9/1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15264</th>\n",
       "      <td>2099862</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lorenza Rush</td>\n",
       "      <td>Riverside</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>92501</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>10/12/1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15265</th>\n",
       "      <td>2099937</td>\n",
       "      <td>Male</td>\n",
       "      <td>Zygmunt Kaminski</td>\n",
       "      <td>Bloomfield Township</td>\n",
       "      <td>MI</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>48302</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>8/18/1965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15266 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustomerKey  Gender               Name                 City State Code  \\\n",
       "0              301  Female      Lilly Harding       WANDEARAH EAST         SA   \n",
       "1              325  Female       Madison Hull           MOUNT BUDD         WA   \n",
       "2              554  Female      Claire Ferres            WINJALLOK        VIC   \n",
       "3              786    Male  Jai Poltpalingada         MIDDLE RIVER         SA   \n",
       "4             1042    Male    Aidan Pankhurst        TAWONGA SOUTH        VIC   \n",
       "...            ...     ...                ...                  ...        ...   \n",
       "15261      2099600  Female     Denisa Dušková              Houston         TX   \n",
       "15262      2099618    Male   Justin Solórzano               Mclean         VA   \n",
       "15263      2099758    Male    Svend Petrussen           Wilmington         NC   \n",
       "15264      2099862  Female       Lorenza Rush            Riverside         CA   \n",
       "15265      2099937    Male   Zygmunt Kaminski  Bloomfield Township         MI   \n",
       "\n",
       "                   State Zip Code        Country      Continent    Birthday  \n",
       "0        South Australia     5523      Australia      Australia    7/3/1939  \n",
       "1      Western Australia     6522      Australia      Australia   9/27/1979  \n",
       "2               Victoria     3380      Australia      Australia   5/26/1947  \n",
       "3        South Australia     5223      Australia      Australia   9/17/1957  \n",
       "4               Victoria     3698      Australia      Australia  11/19/1965  \n",
       "...                  ...      ...            ...            ...         ...  \n",
       "15261              Texas    77017  United States  North America   3/25/1936  \n",
       "15262           Virginia    22101  United States  North America   2/16/1992  \n",
       "15263     North Carolina    28405  United States  North America   11/9/1937  \n",
       "15264         California    92501  United States  North America  10/12/1937  \n",
       "15265           Michigan    48302  United States  North America   8/18/1965  \n",
       "\n",
       "[15266 rows x 10 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47fe68fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    South Australia     5523  Australia  Australia    7/3/1939  \n",
      "1  Western Australia     6522  Australia  Australia   9/27/1979  \n",
      "2           Victoria     3380  Australia  Australia   5/26/1947  \n",
      "3    South Australia     5223  Australia  Australia   9/17/1957  \n",
      "4           Victoria     3698  Australia  Australia  11/19/1965  \n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "552b173c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerKey</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State Code</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Birthday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lilly Harding</td>\n",
       "      <td>WANDEARAH EAST</td>\n",
       "      <td>SA</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>5523</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>7/3/1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325</td>\n",
       "      <td>Female</td>\n",
       "      <td>Madison Hull</td>\n",
       "      <td>MOUNT BUDD</td>\n",
       "      <td>WA</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>6522</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>9/27/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>554</td>\n",
       "      <td>Female</td>\n",
       "      <td>Claire Ferres</td>\n",
       "      <td>WINJALLOK</td>\n",
       "      <td>VIC</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3380</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>5/26/1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jai Poltpalingada</td>\n",
       "      <td>MIDDLE RIVER</td>\n",
       "      <td>SA</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>5223</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>9/17/1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1042</td>\n",
       "      <td>Male</td>\n",
       "      <td>Aidan Pankhurst</td>\n",
       "      <td>TAWONGA SOUTH</td>\n",
       "      <td>VIC</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3698</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>11/19/1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15261</th>\n",
       "      <td>2099600</td>\n",
       "      <td>Female</td>\n",
       "      <td>Denisa Dušková</td>\n",
       "      <td>Houston</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77017</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>3/25/1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15262</th>\n",
       "      <td>2099618</td>\n",
       "      <td>Male</td>\n",
       "      <td>Justin Solórzano</td>\n",
       "      <td>Mclean</td>\n",
       "      <td>VA</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>22101</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>2/16/1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15263</th>\n",
       "      <td>2099758</td>\n",
       "      <td>Male</td>\n",
       "      <td>Svend Petrussen</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>NC</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28405</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>11/9/1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15264</th>\n",
       "      <td>2099862</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lorenza Rush</td>\n",
       "      <td>Riverside</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>92501</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>10/12/1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15265</th>\n",
       "      <td>2099937</td>\n",
       "      <td>Male</td>\n",
       "      <td>Zygmunt Kaminski</td>\n",
       "      <td>Bloomfield Township</td>\n",
       "      <td>MI</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>48302</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>8/18/1965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15266 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustomerKey  Gender               Name                 City State Code  \\\n",
       "0              301  Female      Lilly Harding       WANDEARAH EAST         SA   \n",
       "1              325  Female       Madison Hull           MOUNT BUDD         WA   \n",
       "2              554  Female      Claire Ferres            WINJALLOK        VIC   \n",
       "3              786    Male  Jai Poltpalingada         MIDDLE RIVER         SA   \n",
       "4             1042    Male    Aidan Pankhurst        TAWONGA SOUTH        VIC   \n",
       "...            ...     ...                ...                  ...        ...   \n",
       "15261      2099600  Female     Denisa Dušková              Houston         TX   \n",
       "15262      2099618    Male   Justin Solórzano               Mclean         VA   \n",
       "15263      2099758    Male    Svend Petrussen           Wilmington         NC   \n",
       "15264      2099862  Female       Lorenza Rush            Riverside         CA   \n",
       "15265      2099937    Male   Zygmunt Kaminski  Bloomfield Township         MI   \n",
       "\n",
       "                   State Zip Code        Country      Continent    Birthday  \n",
       "0        South Australia     5523      Australia      Australia    7/3/1939  \n",
       "1      Western Australia     6522      Australia      Australia   9/27/1979  \n",
       "2               Victoria     3380      Australia      Australia   5/26/1947  \n",
       "3        South Australia     5223      Australia      Australia   9/17/1957  \n",
       "4               Victoria     3698      Australia      Australia  11/19/1965  \n",
       "...                  ...      ...            ...            ...         ...  \n",
       "15261              Texas    77017  United States  North America   3/25/1936  \n",
       "15262           Virginia    22101  United States  North America   2/16/1992  \n",
       "15263     North Carolina    28405  United States  North America   11/9/1937  \n",
       "15264         California    92501  United States  North America  10/12/1937  \n",
       "15265           Michigan    48302  United States  North America   8/18/1965  \n",
       "\n",
       "[15266 rows x 10 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9cded4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5267989e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfc in position 5460: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the CSV data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/shadivaz/Desktop/PROJECT 2/Customers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a dictionary to map state names to state codes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m state_code_dict \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState Code\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState Code\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xfc in position 5460: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv')\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'].fillna(df['State'].map(state_code_dict), inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3ff4bc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'errors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the CSV data with Unicode error handling\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/shadivaz/Desktop/PROJECT 2/Customers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a dictionary to map state names to state codes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m state_code_dict \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState Code\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState Code\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'errors'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data with Unicode error handling\n",
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding='utf-8', errors='replace')\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'].fillna(df['State'].map(state_code_dict), inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a2611be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CustomerKey  Gender               Name                 City State Code  \\\n",
      "0              301  Female      Lilly Harding       WANDEARAH EAST         SA   \n",
      "1              325  Female       Madison Hull           MOUNT BUDD         WA   \n",
      "2              554  Female      Claire Ferres            WINJALLOK        VIC   \n",
      "3              786    Male  Jai Poltpalingada         MIDDLE RIVER         SA   \n",
      "4             1042    Male    Aidan Pankhurst        TAWONGA SOUTH        VIC   \n",
      "...            ...     ...                ...                  ...        ...   \n",
      "15261      2099600  Female     Denisa Du�kov�              Houston         TX   \n",
      "15262      2099618    Male   Justin Sol�rzano               Mclean         VA   \n",
      "15263      2099758    Male    Svend Petrussen           Wilmington         NC   \n",
      "15264      2099862  Female       Lorenza Rush            Riverside         CA   \n",
      "15265      2099937    Male   Zygmunt Kaminski  Bloomfield Township         MI   \n",
      "\n",
      "                   State Zip Code        Country      Continent    Birthday  \n",
      "0        South Australia     5523      Australia      Australia    7/3/1939  \n",
      "1      Western Australia     6522      Australia      Australia   9/27/1979  \n",
      "2               Victoria     3380      Australia      Australia   5/26/1947  \n",
      "3        South Australia     5223      Australia      Australia   9/17/1957  \n",
      "4               Victoria     3698      Australia      Australia  11/19/1965  \n",
      "...                  ...      ...            ...            ...         ...  \n",
      "15261              Texas    77017  United States  North America   3/25/1936  \n",
      "15262           Virginia    22101  United States  North America   2/16/1992  \n",
      "15263     North Carolina    28405  United States  North America   11/9/1937  \n",
      "15264         California    92501  United States  North America  10/12/1937  \n",
      "15265           Michigan    48302  United States  North America   8/18/1965  \n",
      "\n",
      "[15266 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the CSV file with error handling for Unicode\n",
    "with open('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding='utf-8', errors='replace') as file:\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'].fillna(df['State'].map(state_code_dict), inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f72af02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28d853e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       State State Code\n",
      "5304  NAPOLI        NaN\n",
      "5316  NAPOLI        NaN\n",
      "5372  NAPOLI        NaN\n",
      "5377  NAPOLI        NaN\n",
      "5378  NAPOLI        NaN\n",
      "5485  NAPOLI        NaN\n",
      "5525  NAPOLI        NaN\n",
      "5531  NAPOLI        NaN\n",
      "5631  NAPOLI        NaN\n",
      "5695  NAPOLI        NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the CSV file with error handling for Unicode\n",
    "with open('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding='utf-8', errors='replace') as file:\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "# Standardize state names to uppercase\n",
    "df['State'] = df['State'].str.upper()\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'].fillna(df['State'].map(state_code_dict), inplace=True)\n",
    "\n",
    "# Identify any remaining missing 'State Code' entries\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "\n",
    "# Print out the rows with missing state codes for manual review\n",
    "print(missing_state_codes[['State', 'State Code']])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV if needed\n",
    "# df.to_csv('/path/to/save/updated_customers.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b24d7a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [State, State Code]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with error handling for Unicode\n",
    "with open('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding='utf-8', errors='replace') as file:\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "# Standardize state names to uppercase\n",
    "df['State'] = df['State'].str.upper()\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'].fillna(df['State'].map(state_code_dict), inplace=True)\n",
    "\n",
    "# Manually set the state code for \"NAPOLI\" if still missing\n",
    "df.loc[df['State'] == 'NAPOLI', 'State Code'] = 'NA'  # Replace 'NA' with the correct code for NAPOLI\n",
    "\n",
    "# Verify if all missing values have been filled\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "print(missing_state_codes[['State', 'State Code']])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV if needed\n",
    "df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5aef8125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15266 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "802ad4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All missing 'State Code' values have been filled.\n"
     ]
    }
   ],
   "source": [
    "# Check again for missing state codes\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "\n",
    "if missing_state_codes.empty:\n",
    "    print(\"All missing 'State Code' values have been filled.\")\n",
    "else:\n",
    "    print(\"There are still some missing 'State Code' values:\")\n",
    "    print(missing_state_codes[['State', 'State Code']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "08135cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "00eaaa67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Code Dictionary: {'SOUTH AUSTRALIA': 'SA', 'WESTERN AUSTRALIA': 'WA', 'VICTORIA': 'VIC', 'QUEENSLAND': 'QLD', 'NORTHERN TERRITORY': 'NT', 'NEW SOUTH WALES': 'NSW', 'TASMANIA': 'TAS', 'AUSTRALIAN CAPITAL TERRITORY': 'ACT', 'BRITISH COLUMBIA': 'BC', 'QUEBEC': 'QC', 'ONTARIO': 'ON', 'ALBERTA': 'AB', 'NOVA SCOTIA': 'NS', 'SASKATCHEWAN': 'SK', 'NUNAVUT': 'NU', 'PRINCE EDWARD ISLAND': 'PE', 'MANITOBA': 'MB', 'NEWFOUNDLAND AND LABRADOR': 'NL', 'YUKON': 'YT', 'NORTHWEST TERRITORIES': 'NT', 'NEW BRUNSWICK': 'NB', 'BRANDENBURG': 'BB', 'RHEINLAND-PFALZ': 'RP', 'FREISTAAT BAYERN': 'BY', 'BADEN-W�RTTEMBERG': 'BW', 'NORDRHEIN-WESTFALEN': 'NW', 'NIEDERSACHSEN': 'NI', 'SACHSEN-ANHALT': 'ST', 'MECKLENBURG-VORPOMMERN': 'MV', 'FREISTAAT SACHSEN': 'SN', 'FREISTAAT TH�RINGEN': 'TH', 'BERLIN': 'BE', 'HESSEN': 'HE', 'SAARLAND': 'SL', 'SCHLESWIG-HOLSTEIN': 'SH', 'FREIE HANSESTADT BREMEN': 'HB', 'HAMBURG': 'HH', 'RH�NE-ALPES': 'RA', '�LE-DE-FRANCE': 'IL', 'GUYANE': 'GY', 'ALSACE': 'AL', 'AQUITAINE': 'AQ', 'CHAMPAGNE-ARDENNE': 'CA', 'NORD-PAS-DE-CALAIS': 'NP', 'GUADELOUPE': 'GD', 'FRANCHE-COMT�': 'FC', \"PROVENCE-ALPES-C�TE D'AZUR\": 'PA', 'POITOU-CHARENTES': 'PC', 'LIMOUSIN': 'LI', 'MIDI-PYR�N�ES': 'MP', 'PICARDIE': 'PI', 'HAUTE-NORMANDIE': 'HN', 'AUVERGNE': 'AU', 'PAYS DE LA LOIRE': 'PL', 'LORRAINE': 'LO', 'BASSE-NORMANDIE': 'BN', 'CENTRE': 'CE', 'LANGUEDOC-ROUSSILLON': 'LN', 'MARTINIQUE': 'MQ', 'BOURGOGNE': 'BO', 'BRETAGNE': 'BR', 'CORSE': 'CO', 'MAYOTTE': 'MY', 'REGGIO CALABRIA': 'RC', 'BENEVENTO': 'BN', 'VICENZA': 'VI', 'FERRARA': 'FE', 'ROMA': 'RM', 'AGRIGENTO': 'AG', 'IMPERIA': 'IM', 'MILANO': 'MI', 'PARMA': 'PR', 'BERGAMO': 'BG', 'RAGUSA': 'RG', 'PORDENONE': 'PN', 'SAVONA': 'SV', 'LUCCA': 'LU', 'CUNEO': 'CN', \"L'AQUILA\": 'AQ', 'TRENTO': 'TN', 'LECCE': 'LE', 'PADOVA': 'PD', 'BOLOGNA': 'BO', 'SALERNO': 'SA', 'BIELLA': 'BI', 'CHIETI': 'CH', 'GENOVA': 'GE', 'TORINO': 'TO', 'VIBO VALENTIA': 'VV', 'CATANZARO': 'CZ', 'ANCONA': 'AN', 'PERUGIA': 'PG', 'LATINA': 'LT', 'BELLUNO': 'BL', 'TREVISO': 'TV', 'PAVIA': 'PV', 'MANTOVA': 'MN', 'VARESE': 'VA', 'PISTOIA': 'PT', 'NUORO': 'NU', 'SIENA': 'SI', 'PIACENZA': 'PC', 'MASSA': 'MS', 'CATANIA': 'CT', 'BRESCIA': 'BS', 'SASSARI': 'SS', 'ROVIGO': 'RO', 'CREMONA': 'CR', 'FIRENZE': 'FI', 'GROSSETO': 'GR', 'CASERTA': 'CE', 'ISERNIA': 'IS', 'SONDRIO': 'SO', 'VENEZIA': 'VE', 'ORISTANO': 'OR', 'MESSINA': 'ME', 'LIVORNO': 'LI', 'PALERMO': 'PA', 'PISA': 'PI', 'VERONA': 'VR', 'COMO': 'CO', 'COSENZA': 'CS', 'BOLZANO': 'BZ', 'NOVARA': 'NO', 'AVELLINO': 'AV', 'TARANTO': 'TA', 'VERCELLI': 'VC', 'GORIZIA': 'GO', 'MODENA': 'MO', 'FROSINONE': 'FR', 'FOGGIA': 'FG', 'ALESSANDRIA': 'AL', 'TERAMO': 'TE', 'BARI': 'BA', 'UDINE': 'UD', 'ASCOLI PICENO': 'AP', 'TRAPANI': 'TP', 'REGGIO EMILIA': 'RE', 'CAGLIARI': 'CA', 'PRATO': 'PO', 'ASTI': 'AT', 'SIRACUSA': 'SR', 'RIETI': 'RI', 'TRIESTE': 'TS', 'CROTONE': 'KR', 'MATERA': 'MT', 'POTENZA': 'PZ', 'MACERATA': 'MC', 'VITERBO': 'VT', 'AREZZO': 'AR', 'AOSTA': 'AO', 'CAMPOBASSO': 'CB', 'BRINDISI': 'BR', 'LECCO': 'LC', 'RAVENNA': 'RA', 'LA SPEZIA': 'SP', 'PESCARA': 'PE', 'RIMINI': 'RN', 'FORLI': 'FO', 'TERMI': 'TR', 'LIMBURG': 'LI', 'UTRECHT': 'UT', 'NOORD-HOLLAND': 'NH', 'NOORD-BRABANT': 'NB', 'GELDERLAND': 'GE', 'FRIESLAND': 'FR', 'GRONINGEN': 'GR', 'DRENTHE': 'DR', 'ZUID-HOLLAND': 'ZH', 'FLEVOLAND': 'FL', 'OVERIJSSEL': 'OV', 'ZEELAND': 'ZE', 'FALKIRK': 'Falkirk', 'CEREDIGION': 'Ceredigion', 'NORTH EAST LINCOLNSHIRE': 'North East Lincolnshire', 'ABERDEENSHIRE': 'Aberdeenshire', 'YORK': 'York', 'PEMBROKESHIRE': 'Pembrokeshire', 'LEICESTER': 'Leicester', 'HIGHLAND': 'Highland', 'TENDRING': 'Tendring', 'HORSHAM': 'Horsham', 'NEWPORT': 'Newport', 'BRISTOL': 'Bristol', 'NEWARK AND SHERWOOD': 'Newark and Sherwood', 'ARGYLLSHIRE': 'Argyllshire', 'LINCOLN': 'Lincoln', 'TAMWORTH': 'Tamworth', 'FYLDE': 'Fylde', 'LEWES': 'Lewes', 'RHONDDA CYNON TAF': 'Rhondda Cynon Taf', 'BROMSGROVE': 'Bromsgrove', 'RIPON': 'Ripon', 'CORNWALL': 'Cornwall', 'SOUTH LANARKSHIRE': 'South Lanarkshire', 'SHROPSHIRE': 'Shropshire', 'PERTH AND KINROSS': 'Perth and Kinross', 'CRAWLEY': 'Crawley', 'STAFFORDSHIRE': 'Staffordshire', 'MENDIP': 'Mendip', 'FOREST HEATH': 'Forest Heath', 'MORAY': 'Moray', 'BRACKNELL FOREST': 'Bracknell Forest', 'ANGLESEY': 'Anglesey', 'BOLSOVER': 'Bolsover', 'CALDERDALE': 'Calderdale', 'ASHFORD': 'Ashford', 'SUSSEX': 'Sussex', 'DARLINGTON': 'Darlington', 'EAST DEVON': 'East Devon', 'MONMOUTHSHIRE': 'Monmouthshire', 'GLOUCESTER': 'Gloucester', 'MID DEVON': 'Mid Devon', 'SOMERSET': 'Somerset', 'HEREFORD': 'Hereford', 'BATH AND NORTH EAST SOMERSET': 'Bath and North East Somerset', 'BASSETLAW': 'Bassetlaw', 'CHRISTCHURCH': 'Christchurch', 'WEST BERKSHIRE': 'West Berkshire', 'GWYNEDD': 'Gwynedd', 'SUFFOLK': 'Suffolk', 'TEWKESBURY': 'Tewkesbury', 'COLCHESTER': 'Colchester', 'LLANDRINDOD WELLS': 'Llandrindod Wells', 'HARROGATE': 'Harrogate', 'WINCHESTER': 'Winchester', 'ANGUS': 'Angus', 'DERBYSHIRE DALES': 'Derbyshire Dales', 'DACORUM': 'Dacorum', 'SUFFOLK COASTAL': 'Suffolk Coastal', 'WILTSHIRE': 'Wiltshire', 'LEEDS': 'Leeds', 'KENNET': 'Kennet', 'HAMPSHIRE': 'Hampshire', 'NORFOLK': 'Norfolk', 'NORTHUMBERLAND': 'Northumberland', 'DONCASTER': 'Doncaster', 'PURBECK': 'Purbeck', 'WYRE FOREST': 'Wyre Forest', 'REDCAR & CLEVELAND': 'Redcar & Cleveland', 'WEST DORSET': 'West Dorset', 'IPSWICH': 'Ipswich', 'POWYS': 'Powys', 'MIDLOTHIAN': 'Midlothian', 'FIFE': 'Fife', 'NORTH AYRSHIRE': 'North Ayrshire', 'CARMARTHENSHIRE': 'Carmarthenshire', 'BIRMINGHAM': 'Birmingham', 'SOUTH OXFORDSHIRE': 'South Oxfordshire', 'NORTH YORKSHIRE': 'North Yorkshire', 'CHESHIRE WEST AND CHESTER': 'Cheshire West and Chester', 'SCOTTISH BORDERS': 'Scottish Borders', 'LICHFIELD': 'Lichfield', 'RUSHCLIFFE': 'Rushcliffe', 'ARUN': 'Arun', 'DUMFRIESSHIRE': 'Dumfriesshire', 'WAKEFIELD': 'Wakefield', 'DENBIGHSHIRE': 'Denbighshire', 'PLYMOUTH': 'Plymouth', 'ST EDMUNDSBURY': 'St Edmundsbury', 'EDINBURGH': 'Edinburgh', 'LIVERPOOL': 'Liverpool', 'KENT': 'Kent', 'WARWICK': 'Warwick', 'GEDLING': 'Gedling', 'SHETLAND': 'Shetland', 'FLINTSHIRE': 'Flintshire', 'GRAVESHAM': 'Gravesham', 'CENTRAL BEDFORDSHIRE': 'Central Bedfordshire', 'NORTH DORSET': 'North Dorset', 'LANCASTER': 'Lancaster', 'BRECKLAND': 'Breckland', 'EAST RIDING OF YORKSHIRE': 'East Riding of Yorkshire', 'EXETER': 'Exeter', 'VALE OF WHITE HORSE': 'Vale of White Horse', 'CHERWELL': 'Cherwell', 'SOUTH HOLLAND': 'South Holland', 'SOUTH LAKELAND': 'South Lakeland', 'STROUD': 'Stroud', 'ORKNEY ISLANDS': 'Orkney Islands', 'WEST LINDSEY': 'West Lindsey', 'STRATFORD-ON-AVON': 'Stratford-on-Avon', 'WEST OXFORDSHIRE': 'West Oxfordshire', 'BEDFORD': 'Bedford', 'ROTHER': 'Rother', 'ISLE OF MAN': 'Isle of Man', 'SWINDON': 'Swindon', 'CHARNWOOD': 'Charnwood', 'WAVERLEY': 'Waverley', 'WOLVERHAMPTON': 'Wolverhampton', 'AYLESBURY VALE': 'Aylesbury Vale', 'MERTON': 'Merton', 'SEVENOAKS': 'Sevenoaks', 'ALLERDALE': 'Allerdale', 'BOLTON': 'Bolton', 'SELBY': 'Selby', 'BERKSHIRE': 'Berkshire', 'COPELAND': 'Copeland', 'CHELMSFORD': 'Chelmsford', 'SOUTH KESTEVEN': 'South Kesteven', 'CHESHIRE EAST': 'Cheshire East', 'BOSTON': 'Boston', 'COUNTY DURHAM': 'County Durham', 'MID SUFFOLK': 'Mid Suffolk', 'WANDSWORTH': 'Wandsworth', 'WEST DUNBARTONSHIRE': 'West Dunbartonshire', 'EREWASH': 'Erewash', 'BABERGH': 'Babergh', 'SOUTH HAMS': 'South Hams', 'WIGAN': 'Wigan', 'SOUTH AYRSHIRE': 'South Ayrshire', 'ROTHERHAM': 'Rotherham', 'ISLE OF WIGHT': 'Isle of Wight', 'COTSWOLD': 'Cotswold', 'WORCESTER': 'Worcester', 'RUTLAND': 'Rutland', 'EAST NORTHAMPTONSHIRE': 'East Northamptonshire', 'RUGBY': 'Rugby', 'STIRLING': 'Stirling', 'NORTH KESTEVEN': 'North Kesteven', 'COMHAIRLE NAN EILEAN SIAR': 'Comhairle nan Eilean Siar', 'WALSALL': 'Walsall', 'KNOWSLEY': 'Knowsley', 'SOUTH SOMERSET': 'South Somerset', 'NORTH LINCOLNSHIRE': 'North Lincolnshire', 'SOUTH NORFOLK': 'South Norfolk', 'ABERDEEN': 'Aberdeen', 'WELWYN HATFIELD': 'Welwyn Hatfield', 'KIRKCUDBRIGHTSHIRE': 'Kirkcudbrightshire', 'CARLISLE': 'Carlisle', 'HILLINGDON': 'Hillingdon', 'WEST NORFOLK': 'West Norfolk', 'KIRKLEES': 'Kirklees', 'NEW FOREST': 'New Forest', 'SWANSEA': 'Swansea', 'CRAVEN': 'Craven', 'CAMDEN': 'Camden', 'NORTH SOMERSET': 'North Somerset', 'TEIGNBRIDGE': 'Teignbridge', 'MELTON': 'Melton', 'CONWY': 'Conwy', 'ENFIELD': 'Enfield', 'GLASGOW': 'Glasgow', 'BRAINTREE': 'Braintree', 'CHICHESTER': 'Chichester', 'NORTH HERTFORDSHIRE': 'North Hertfordshire', 'EAST LOTHIAN': 'East Lothian', 'RENFREWSHIRE': 'Renfrewshire', 'EDEN': 'Eden', 'UTTLESFORD': 'Uttlesford', 'MID SUSSEX': 'Mid Sussex', 'PETERBOROUGH': 'Peterborough', 'ASHFIELD': 'Ashfield', 'REDBRIDGE': 'Redbridge', 'SHEFFIELD': 'Sheffield', 'NEWCASTLE': 'Newcastle', 'HARLOW': 'Harlow', 'EAST HAMPSHIRE': 'East Hampshire', 'CAERPHILLY': 'Caerphilly', 'NORTH WARWICKSHIRE': 'North Warwickshire', 'BRENTWOOD': 'Brentwood', 'TAMESIDE': 'Tameside', 'BRIGHTON AND HOVE': 'Brighton and Hove', 'ROSSENDALE': 'Rossendale', 'SWALE': 'Swale', 'SUTTON': 'Sutton', 'HUNTINGDONSHIRE': 'Huntingdonshire', 'ST ALBANS': 'St Albans', 'DUDLEY': 'Dudley', 'EAST HERTFORDSHIRE': 'East Hertfordshire', 'GUILDFORD': 'Guildford', 'CAMBRIDGE': 'Cambridge', 'WOKING': 'Woking', 'DAVENTRY': 'Daventry', 'VALE OF GLAMORGAN': 'Vale of Glamorgan', 'EAST AYRSHIRE': 'East Ayrshire', 'BRADFORD': 'Bradford', 'SOUTH BUCKINGHAMSHIRE': 'South Buckinghamshire', 'MERTHYR TYDFIL': 'Merthyr Tydfil', 'WEST LOTHIAN': 'West Lothian', 'EAST LINDSEY': 'East Lindsey', 'HAMBLETON': 'Hambleton', 'TOWER HAMLETS': 'Tower Hamlets', 'TEST VALLEY': 'Test Valley', 'DARTFORD': 'Dartford', 'SUNDERLAND': 'Sunderland', 'MEDWAY': 'Medway', 'ROCHDALE': 'Rochdale', 'BURY': 'Bury', 'OXFORD': 'Oxford', 'BRIDGEND': 'Bridgend', 'SOUTH DERBYSHIRE': 'South Derbyshire', 'MILTON KEYNES': 'Milton Keynes', 'SOUTHAMPTON': 'Southampton', 'WOKINGHAM': 'Wokingham', 'NOTTINGHAM': 'Nottingham', 'STEVENAGE': 'Stevenage', 'WIRRAL': 'Wirral', 'DUNDEE': 'Dundee', 'AMBER VALLEY': 'Amber Valley', 'HARROW': 'Harrow', 'EAST DORSET': 'East Dorset', 'MAIDSTONE': 'Maidstone', 'WIGTOWNSHIRE': 'Wigtownshire', 'GATESHEAD': 'Gateshead', 'TELFORD AND WREKIN': 'Telford and Wrekin', 'WELLINGBOROUGH': 'Wellingborough', 'RUNNYMEDE': 'Runnymede', 'WREXHAM': 'Wrexham', 'CANNOCK CHASE': 'Cannock Chase', 'KENSINGTON AND CHELSEA': 'Kensington and Chelsea', 'SOUTH GLOUCESTERSHIRE': 'South Gloucestershire', 'WYCOMBE': 'Wycombe', 'SOUTH STAFFORDSHIRE': 'South Staffordshire', 'SANDWELL': 'Sandwell', 'BROMLEY': 'Bromley', 'TORRIDGE': 'Torridge', 'BURNLEY': 'Burnley', 'SEFTON': 'Sefton', 'RIBBLE VALLEY': 'Ribble Valley', 'CHILTERN': 'Chiltern', 'HASTINGS': 'Hastings', 'EAST STAFFORDSHIRE': 'East Staffordshire', 'BARNET': 'Barnet', 'REIGATE AND BANSTEAD': 'Reigate and Banstead', 'BASILDON': 'Basildon', 'TANDRIDGE': 'Tandridge', 'WESTMINSTER': 'Westminster', 'WARRINGTON': 'Warrington', 'WEST LANCASHIRE': 'West Lancashire', 'PRESTON': 'Preston', 'CANTERBURY': 'Canterbury', 'WEALDEN': 'Wealden', 'HAVERING': 'Havering', 'WEST DEVON': 'West Devon', 'ISLINGTON': 'Islington', 'WYRE': 'Wyre', 'KINROSS-SHIRE': 'Kinross-Shire', 'WAVENEY': 'Waveney', 'NORTH LANARKSHIRE': 'North Lanarkshire', 'ELY': 'Ely', 'NEATH PORT TALBOT': 'Neath Port Talbot', 'NUNEATON & BEDWORTH': 'Nuneaton & Bedworth', 'CHESTERFIELD': 'Chesterfield', 'MOLE VALLEY': 'Mole Valley', 'SOUTH NORTHAMPTONSHIRE': 'South Northamptonshire', 'BROXBOURNE': 'Broxbourne', 'CARDIFF': 'Cardiff', 'HACKNEY': 'Hackney', 'REDDITCH': 'Redditch', 'TUNBRIDGE WELLS': 'Tunbridge Wells', 'HARINGEY': 'Haringey', 'MALVERN HILLS': 'Malvern Hills', 'BROXTOWE': 'Broxtowe', 'SPELTHORNE': 'Spelthorne', 'COVENTRY': 'Coventry', 'NEWMARKET': 'Newmarket', 'LANARKSHIRE': 'Lanarkshire', 'EAST DUNBARTONSHIRE': 'East Dunbartonshire', 'STOCKTON-ON-TEES': 'Stockton-on-Tees', 'CALIFORNIA': 'CA', 'NORTH CAROLINA': 'NC', 'MARYLAND': 'MD', 'HAWAII': 'HI', 'COLORADO': 'CO', 'FLORIDA': 'FL', 'MICHIGAN': 'MI', 'ALABAMA': 'AL', 'MISSOURI': 'MO', 'ILLINOIS': 'IL', 'NEW YORK': 'NY', 'TEXAS': 'TX', 'MISSISSIPPI': 'MS', 'GEORGIA': 'GA', 'UTAH': 'UT', 'DELAWARE': 'DE', 'WISCONSIN': 'WI', 'NEW HAMPSHIRE': 'NH', 'KANSAS': 'KS', 'VERMONT': 'VT', 'NEVADA': 'NV', 'PENNSYLVANIA': 'PA', 'MASSACHUSETTS': 'MA', 'WASHINGTON': 'WA', 'NEW JERSEY': 'NJ', 'VIRGINIA': 'VA', 'OHIO': 'OH', 'IOWA': 'IA', 'KENTUCKY': 'KY', 'WASHINGTON DC': 'DC', 'ARKANSAS': 'AR', 'OKLAHOMA': 'OK', 'OREGON': 'OR', 'MAINE': 'ME', 'IDAHO': 'ID', 'MINNESOTA': 'MN', 'ARIZONA': 'AZ', 'INDIANA': 'IN', 'CONNECTICUT': 'CT', 'WEST VIRGINIA': 'WV', 'NEBRASKA': 'NE', 'TENNESSEE': 'TN', 'LOUISIANA': 'LA', 'ALASKA': 'AK', 'NORTH DAKOTA': 'ND', 'MONTANA': 'MT', 'SOUTH CAROLINA': 'SC', 'NEW MEXICO': 'NM', 'RHODE ISLAND': 'RI', 'SOUTH DAKOTA': 'SD', 'WYOMING': 'WY'}\n",
      "Missing state codes before manual fix: 10\n",
      "Missing state codes after manual fix: 0\n",
      "Empty DataFrame\n",
      "Columns: [State, State Code]\n",
      "Index: []\n",
      "File saved to /Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with error handling for Unicode\n",
    "with open('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding='utf-8', errors='replace') as file:\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "# Standardize state names to uppercase\n",
    "df['State'] = df['State'].str.upper()\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Display the state code dictionary for debugging\n",
    "print(\"State Code Dictionary:\", state_code_dict)\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'] = df['State Code'].fillna(df['State'].map(state_code_dict))\n",
    "\n",
    "# Check how many values are still missing\n",
    "missing_count_before = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes before manual fix: {missing_count_before}\")\n",
    "\n",
    "# Manually set the state code for \"NAPOLI\" if still missing\n",
    "df.loc[df['State'] == 'NAPOLI', 'State Code'] = df.loc[df['State'] == 'NAPOLI', 'State Code'].fillna('NA')\n",
    "\n",
    "# Check how many values are still missing after the manual fix\n",
    "missing_count_after = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes after manual fix: {missing_count_after}\")\n",
    "\n",
    "# Display the DataFrame rows where state code is missing\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "print(missing_state_codes[['State', 'State Code']])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV\n",
    "output_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "114668ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92c144e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Code Dictionary: {'SOUTH AUSTRALIA': 'SA', 'WESTERN AUSTRALIA': 'WA', 'VICTORIA': 'VIC', 'QUEENSLAND': 'QLD', 'NORTHERN TERRITORY': 'NT', 'NEW SOUTH WALES': 'NSW', 'TASMANIA': 'TAS', 'AUSTRALIAN CAPITAL TERRITORY': 'ACT', 'BRITISH COLUMBIA': 'BC', 'QUEBEC': 'QC', 'ONTARIO': 'ON', 'ALBERTA': 'AB', 'NOVA SCOTIA': 'NS', 'SASKATCHEWAN': 'SK', 'NUNAVUT': 'NU', 'PRINCE EDWARD ISLAND': 'PE', 'MANITOBA': 'MB', 'NEWFOUNDLAND AND LABRADOR': 'NL', 'YUKON': 'YT', 'NORTHWEST TERRITORIES': 'NT', 'NEW BRUNSWICK': 'NB', 'BRANDENBURG': 'BB', 'RHEINLAND-PFALZ': 'RP', 'FREISTAAT BAYERN': 'BY', 'BADEN-W�RTTEMBERG': 'BW', 'NORDRHEIN-WESTFALEN': 'NW', 'NIEDERSACHSEN': 'NI', 'SACHSEN-ANHALT': 'ST', 'MECKLENBURG-VORPOMMERN': 'MV', 'FREISTAAT SACHSEN': 'SN', 'FREISTAAT TH�RINGEN': 'TH', 'BERLIN': 'BE', 'HESSEN': 'HE', 'SAARLAND': 'SL', 'SCHLESWIG-HOLSTEIN': 'SH', 'FREIE HANSESTADT BREMEN': 'HB', 'HAMBURG': 'HH', 'RH�NE-ALPES': 'RA', '�LE-DE-FRANCE': 'IL', 'GUYANE': 'GY', 'ALSACE': 'AL', 'AQUITAINE': 'AQ', 'CHAMPAGNE-ARDENNE': 'CA', 'NORD-PAS-DE-CALAIS': 'NP', 'GUADELOUPE': 'GD', 'FRANCHE-COMT�': 'FC', \"PROVENCE-ALPES-C�TE D'AZUR\": 'PA', 'POITOU-CHARENTES': 'PC', 'LIMOUSIN': 'LI', 'MIDI-PYR�N�ES': 'MP', 'PICARDIE': 'PI', 'HAUTE-NORMANDIE': 'HN', 'AUVERGNE': 'AU', 'PAYS DE LA LOIRE': 'PL', 'LORRAINE': 'LO', 'BASSE-NORMANDIE': 'BN', 'CENTRE': 'CE', 'LANGUEDOC-ROUSSILLON': 'LN', 'MARTINIQUE': 'MQ', 'BOURGOGNE': 'BO', 'BRETAGNE': 'BR', 'CORSE': 'CO', 'MAYOTTE': 'MY', 'REGGIO CALABRIA': 'RC', 'BENEVENTO': 'BN', 'VICENZA': 'VI', 'FERRARA': 'FE', 'ROMA': 'RM', 'AGRIGENTO': 'AG', 'IMPERIA': 'IM', 'MILANO': 'MI', 'PARMA': 'PR', 'BERGAMO': 'BG', 'RAGUSA': 'RG', 'PORDENONE': 'PN', 'SAVONA': 'SV', 'LUCCA': 'LU', 'CUNEO': 'CN', \"L'AQUILA\": 'AQ', 'TRENTO': 'TN', 'LECCE': 'LE', 'PADOVA': 'PD', 'BOLOGNA': 'BO', 'SALERNO': 'SA', 'BIELLA': 'BI', 'CHIETI': 'CH', 'GENOVA': 'GE', 'TORINO': 'TO', 'VIBO VALENTIA': 'VV', 'CATANZARO': 'CZ', 'ANCONA': 'AN', 'PERUGIA': 'PG', 'LATINA': 'LT', 'BELLUNO': 'BL', 'TREVISO': 'TV', 'PAVIA': 'PV', 'MANTOVA': 'MN', 'VARESE': 'VA', 'PISTOIA': 'PT', 'NUORO': 'NU', 'SIENA': 'SI', 'PIACENZA': 'PC', 'MASSA': 'MS', 'CATANIA': 'CT', 'BRESCIA': 'BS', 'SASSARI': 'SS', 'ROVIGO': 'RO', 'CREMONA': 'CR', 'FIRENZE': 'FI', 'GROSSETO': 'GR', 'CASERTA': 'CE', 'ISERNIA': 'IS', 'SONDRIO': 'SO', 'VENEZIA': 'VE', 'ORISTANO': 'OR', 'MESSINA': 'ME', 'LIVORNO': 'LI', 'PALERMO': 'PA', 'PISA': 'PI', 'VERONA': 'VR', 'COMO': 'CO', 'COSENZA': 'CS', 'BOLZANO': 'BZ', 'NOVARA': 'NO', 'AVELLINO': 'AV', 'TARANTO': 'TA', 'VERCELLI': 'VC', 'GORIZIA': 'GO', 'MODENA': 'MO', 'FROSINONE': 'FR', 'FOGGIA': 'FG', 'ALESSANDRIA': 'AL', 'TERAMO': 'TE', 'BARI': 'BA', 'UDINE': 'UD', 'ASCOLI PICENO': 'AP', 'TRAPANI': 'TP', 'REGGIO EMILIA': 'RE', 'CAGLIARI': 'CA', 'PRATO': 'PO', 'ASTI': 'AT', 'SIRACUSA': 'SR', 'RIETI': 'RI', 'TRIESTE': 'TS', 'CROTONE': 'KR', 'MATERA': 'MT', 'POTENZA': 'PZ', 'MACERATA': 'MC', 'VITERBO': 'VT', 'AREZZO': 'AR', 'AOSTA': 'AO', 'CAMPOBASSO': 'CB', 'BRINDISI': 'BR', 'LECCO': 'LC', 'RAVENNA': 'RA', 'LA SPEZIA': 'SP', 'PESCARA': 'PE', 'RIMINI': 'RN', 'FORLI': 'FO', 'TERMI': 'TR', 'LIMBURG': 'LI', 'UTRECHT': 'UT', 'NOORD-HOLLAND': 'NH', 'NOORD-BRABANT': 'NB', 'GELDERLAND': 'GE', 'FRIESLAND': 'FR', 'GRONINGEN': 'GR', 'DRENTHE': 'DR', 'ZUID-HOLLAND': 'ZH', 'FLEVOLAND': 'FL', 'OVERIJSSEL': 'OV', 'ZEELAND': 'ZE', 'FALKIRK': 'Falkirk', 'CEREDIGION': 'Ceredigion', 'NORTH EAST LINCOLNSHIRE': 'North East Lincolnshire', 'ABERDEENSHIRE': 'Aberdeenshire', 'YORK': 'York', 'PEMBROKESHIRE': 'Pembrokeshire', 'LEICESTER': 'Leicester', 'HIGHLAND': 'Highland', 'TENDRING': 'Tendring', 'HORSHAM': 'Horsham', 'NEWPORT': 'Newport', 'BRISTOL': 'Bristol', 'NEWARK AND SHERWOOD': 'Newark and Sherwood', 'ARGYLLSHIRE': 'Argyllshire', 'LINCOLN': 'Lincoln', 'TAMWORTH': 'Tamworth', 'FYLDE': 'Fylde', 'LEWES': 'Lewes', 'RHONDDA CYNON TAF': 'Rhondda Cynon Taf', 'BROMSGROVE': 'Bromsgrove', 'RIPON': 'Ripon', 'CORNWALL': 'Cornwall', 'SOUTH LANARKSHIRE': 'South Lanarkshire', 'SHROPSHIRE': 'Shropshire', 'PERTH AND KINROSS': 'Perth and Kinross', 'CRAWLEY': 'Crawley', 'STAFFORDSHIRE': 'Staffordshire', 'MENDIP': 'Mendip', 'FOREST HEATH': 'Forest Heath', 'MORAY': 'Moray', 'BRACKNELL FOREST': 'Bracknell Forest', 'ANGLESEY': 'Anglesey', 'BOLSOVER': 'Bolsover', 'CALDERDALE': 'Calderdale', 'ASHFORD': 'Ashford', 'SUSSEX': 'Sussex', 'DARLINGTON': 'Darlington', 'EAST DEVON': 'East Devon', 'MONMOUTHSHIRE': 'Monmouthshire', 'GLOUCESTER': 'Gloucester', 'MID DEVON': 'Mid Devon', 'SOMERSET': 'Somerset', 'HEREFORD': 'Hereford', 'BATH AND NORTH EAST SOMERSET': 'Bath and North East Somerset', 'BASSETLAW': 'Bassetlaw', 'CHRISTCHURCH': 'Christchurch', 'WEST BERKSHIRE': 'West Berkshire', 'GWYNEDD': 'Gwynedd', 'SUFFOLK': 'Suffolk', 'TEWKESBURY': 'Tewkesbury', 'COLCHESTER': 'Colchester', 'LLANDRINDOD WELLS': 'Llandrindod Wells', 'HARROGATE': 'Harrogate', 'WINCHESTER': 'Winchester', 'ANGUS': 'Angus', 'DERBYSHIRE DALES': 'Derbyshire Dales', 'DACORUM': 'Dacorum', 'SUFFOLK COASTAL': 'Suffolk Coastal', 'WILTSHIRE': 'Wiltshire', 'LEEDS': 'Leeds', 'KENNET': 'Kennet', 'HAMPSHIRE': 'Hampshire', 'NORFOLK': 'Norfolk', 'NORTHUMBERLAND': 'Northumberland', 'DONCASTER': 'Doncaster', 'PURBECK': 'Purbeck', 'WYRE FOREST': 'Wyre Forest', 'REDCAR & CLEVELAND': 'Redcar & Cleveland', 'WEST DORSET': 'West Dorset', 'IPSWICH': 'Ipswich', 'POWYS': 'Powys', 'MIDLOTHIAN': 'Midlothian', 'FIFE': 'Fife', 'NORTH AYRSHIRE': 'North Ayrshire', 'CARMARTHENSHIRE': 'Carmarthenshire', 'BIRMINGHAM': 'Birmingham', 'SOUTH OXFORDSHIRE': 'South Oxfordshire', 'NORTH YORKSHIRE': 'North Yorkshire', 'CHESHIRE WEST AND CHESTER': 'Cheshire West and Chester', 'SCOTTISH BORDERS': 'Scottish Borders', 'LICHFIELD': 'Lichfield', 'RUSHCLIFFE': 'Rushcliffe', 'ARUN': 'Arun', 'DUMFRIESSHIRE': 'Dumfriesshire', 'WAKEFIELD': 'Wakefield', 'DENBIGHSHIRE': 'Denbighshire', 'PLYMOUTH': 'Plymouth', 'ST EDMUNDSBURY': 'St Edmundsbury', 'EDINBURGH': 'Edinburgh', 'LIVERPOOL': 'Liverpool', 'KENT': 'Kent', 'WARWICK': 'Warwick', 'GEDLING': 'Gedling', 'SHETLAND': 'Shetland', 'FLINTSHIRE': 'Flintshire', 'GRAVESHAM': 'Gravesham', 'CENTRAL BEDFORDSHIRE': 'Central Bedfordshire', 'NORTH DORSET': 'North Dorset', 'LANCASTER': 'Lancaster', 'BRECKLAND': 'Breckland', 'EAST RIDING OF YORKSHIRE': 'East Riding of Yorkshire', 'EXETER': 'Exeter', 'VALE OF WHITE HORSE': 'Vale of White Horse', 'CHERWELL': 'Cherwell', 'SOUTH HOLLAND': 'South Holland', 'SOUTH LAKELAND': 'South Lakeland', 'STROUD': 'Stroud', 'ORKNEY ISLANDS': 'Orkney Islands', 'WEST LINDSEY': 'West Lindsey', 'STRATFORD-ON-AVON': 'Stratford-on-Avon', 'WEST OXFORDSHIRE': 'West Oxfordshire', 'BEDFORD': 'Bedford', 'ROTHER': 'Rother', 'ISLE OF MAN': 'Isle of Man', 'SWINDON': 'Swindon', 'CHARNWOOD': 'Charnwood', 'WAVERLEY': 'Waverley', 'WOLVERHAMPTON': 'Wolverhampton', 'AYLESBURY VALE': 'Aylesbury Vale', 'MERTON': 'Merton', 'SEVENOAKS': 'Sevenoaks', 'ALLERDALE': 'Allerdale', 'BOLTON': 'Bolton', 'SELBY': 'Selby', 'BERKSHIRE': 'Berkshire', 'COPELAND': 'Copeland', 'CHELMSFORD': 'Chelmsford', 'SOUTH KESTEVEN': 'South Kesteven', 'CHESHIRE EAST': 'Cheshire East', 'BOSTON': 'Boston', 'COUNTY DURHAM': 'County Durham', 'MID SUFFOLK': 'Mid Suffolk', 'WANDSWORTH': 'Wandsworth', 'WEST DUNBARTONSHIRE': 'West Dunbartonshire', 'EREWASH': 'Erewash', 'BABERGH': 'Babergh', 'SOUTH HAMS': 'South Hams', 'WIGAN': 'Wigan', 'SOUTH AYRSHIRE': 'South Ayrshire', 'ROTHERHAM': 'Rotherham', 'ISLE OF WIGHT': 'Isle of Wight', 'COTSWOLD': 'Cotswold', 'WORCESTER': 'Worcester', 'RUTLAND': 'Rutland', 'EAST NORTHAMPTONSHIRE': 'East Northamptonshire', 'RUGBY': 'Rugby', 'STIRLING': 'Stirling', 'NORTH KESTEVEN': 'North Kesteven', 'COMHAIRLE NAN EILEAN SIAR': 'Comhairle nan Eilean Siar', 'WALSALL': 'Walsall', 'KNOWSLEY': 'Knowsley', 'SOUTH SOMERSET': 'South Somerset', 'NORTH LINCOLNSHIRE': 'North Lincolnshire', 'SOUTH NORFOLK': 'South Norfolk', 'ABERDEEN': 'Aberdeen', 'WELWYN HATFIELD': 'Welwyn Hatfield', 'KIRKCUDBRIGHTSHIRE': 'Kirkcudbrightshire', 'CARLISLE': 'Carlisle', 'HILLINGDON': 'Hillingdon', 'WEST NORFOLK': 'West Norfolk', 'KIRKLEES': 'Kirklees', 'NEW FOREST': 'New Forest', 'SWANSEA': 'Swansea', 'CRAVEN': 'Craven', 'CAMDEN': 'Camden', 'NORTH SOMERSET': 'North Somerset', 'TEIGNBRIDGE': 'Teignbridge', 'MELTON': 'Melton', 'CONWY': 'Conwy', 'ENFIELD': 'Enfield', 'GLASGOW': 'Glasgow', 'BRAINTREE': 'Braintree', 'CHICHESTER': 'Chichester', 'NORTH HERTFORDSHIRE': 'North Hertfordshire', 'EAST LOTHIAN': 'East Lothian', 'RENFREWSHIRE': 'Renfrewshire', 'EDEN': 'Eden', 'UTTLESFORD': 'Uttlesford', 'MID SUSSEX': 'Mid Sussex', 'PETERBOROUGH': 'Peterborough', 'ASHFIELD': 'Ashfield', 'REDBRIDGE': 'Redbridge', 'SHEFFIELD': 'Sheffield', 'NEWCASTLE': 'Newcastle', 'HARLOW': 'Harlow', 'EAST HAMPSHIRE': 'East Hampshire', 'CAERPHILLY': 'Caerphilly', 'NORTH WARWICKSHIRE': 'North Warwickshire', 'BRENTWOOD': 'Brentwood', 'TAMESIDE': 'Tameside', 'BRIGHTON AND HOVE': 'Brighton and Hove', 'ROSSENDALE': 'Rossendale', 'SWALE': 'Swale', 'SUTTON': 'Sutton', 'HUNTINGDONSHIRE': 'Huntingdonshire', 'ST ALBANS': 'St Albans', 'DUDLEY': 'Dudley', 'EAST HERTFORDSHIRE': 'East Hertfordshire', 'GUILDFORD': 'Guildford', 'CAMBRIDGE': 'Cambridge', 'WOKING': 'Woking', 'DAVENTRY': 'Daventry', 'VALE OF GLAMORGAN': 'Vale of Glamorgan', 'EAST AYRSHIRE': 'East Ayrshire', 'BRADFORD': 'Bradford', 'SOUTH BUCKINGHAMSHIRE': 'South Buckinghamshire', 'MERTHYR TYDFIL': 'Merthyr Tydfil', 'WEST LOTHIAN': 'West Lothian', 'EAST LINDSEY': 'East Lindsey', 'HAMBLETON': 'Hambleton', 'TOWER HAMLETS': 'Tower Hamlets', 'TEST VALLEY': 'Test Valley', 'DARTFORD': 'Dartford', 'SUNDERLAND': 'Sunderland', 'MEDWAY': 'Medway', 'ROCHDALE': 'Rochdale', 'BURY': 'Bury', 'OXFORD': 'Oxford', 'BRIDGEND': 'Bridgend', 'SOUTH DERBYSHIRE': 'South Derbyshire', 'MILTON KEYNES': 'Milton Keynes', 'SOUTHAMPTON': 'Southampton', 'WOKINGHAM': 'Wokingham', 'NOTTINGHAM': 'Nottingham', 'STEVENAGE': 'Stevenage', 'WIRRAL': 'Wirral', 'DUNDEE': 'Dundee', 'AMBER VALLEY': 'Amber Valley', 'HARROW': 'Harrow', 'EAST DORSET': 'East Dorset', 'MAIDSTONE': 'Maidstone', 'WIGTOWNSHIRE': 'Wigtownshire', 'GATESHEAD': 'Gateshead', 'TELFORD AND WREKIN': 'Telford and Wrekin', 'WELLINGBOROUGH': 'Wellingborough', 'RUNNYMEDE': 'Runnymede', 'WREXHAM': 'Wrexham', 'CANNOCK CHASE': 'Cannock Chase', 'KENSINGTON AND CHELSEA': 'Kensington and Chelsea', 'SOUTH GLOUCESTERSHIRE': 'South Gloucestershire', 'WYCOMBE': 'Wycombe', 'SOUTH STAFFORDSHIRE': 'South Staffordshire', 'SANDWELL': 'Sandwell', 'BROMLEY': 'Bromley', 'TORRIDGE': 'Torridge', 'BURNLEY': 'Burnley', 'SEFTON': 'Sefton', 'RIBBLE VALLEY': 'Ribble Valley', 'CHILTERN': 'Chiltern', 'HASTINGS': 'Hastings', 'EAST STAFFORDSHIRE': 'East Staffordshire', 'BARNET': 'Barnet', 'REIGATE AND BANSTEAD': 'Reigate and Banstead', 'BASILDON': 'Basildon', 'TANDRIDGE': 'Tandridge', 'WESTMINSTER': 'Westminster', 'WARRINGTON': 'Warrington', 'WEST LANCASHIRE': 'West Lancashire', 'PRESTON': 'Preston', 'CANTERBURY': 'Canterbury', 'WEALDEN': 'Wealden', 'HAVERING': 'Havering', 'WEST DEVON': 'West Devon', 'ISLINGTON': 'Islington', 'WYRE': 'Wyre', 'KINROSS-SHIRE': 'Kinross-Shire', 'WAVENEY': 'Waveney', 'NORTH LANARKSHIRE': 'North Lanarkshire', 'ELY': 'Ely', 'NEATH PORT TALBOT': 'Neath Port Talbot', 'NUNEATON & BEDWORTH': 'Nuneaton & Bedworth', 'CHESTERFIELD': 'Chesterfield', 'MOLE VALLEY': 'Mole Valley', 'SOUTH NORTHAMPTONSHIRE': 'South Northamptonshire', 'BROXBOURNE': 'Broxbourne', 'CARDIFF': 'Cardiff', 'HACKNEY': 'Hackney', 'REDDITCH': 'Redditch', 'TUNBRIDGE WELLS': 'Tunbridge Wells', 'HARINGEY': 'Haringey', 'MALVERN HILLS': 'Malvern Hills', 'BROXTOWE': 'Broxtowe', 'SPELTHORNE': 'Spelthorne', 'COVENTRY': 'Coventry', 'NEWMARKET': 'Newmarket', 'LANARKSHIRE': 'Lanarkshire', 'EAST DUNBARTONSHIRE': 'East Dunbartonshire', 'STOCKTON-ON-TEES': 'Stockton-on-Tees', 'CALIFORNIA': 'CA', 'NORTH CAROLINA': 'NC', 'MARYLAND': 'MD', 'HAWAII': 'HI', 'COLORADO': 'CO', 'FLORIDA': 'FL', 'MICHIGAN': 'MI', 'ALABAMA': 'AL', 'MISSOURI': 'MO', 'ILLINOIS': 'IL', 'NEW YORK': 'NY', 'TEXAS': 'TX', 'MISSISSIPPI': 'MS', 'GEORGIA': 'GA', 'UTAH': 'UT', 'DELAWARE': 'DE', 'WISCONSIN': 'WI', 'NEW HAMPSHIRE': 'NH', 'KANSAS': 'KS', 'VERMONT': 'VT', 'NEVADA': 'NV', 'PENNSYLVANIA': 'PA', 'MASSACHUSETTS': 'MA', 'WASHINGTON': 'WA', 'NEW JERSEY': 'NJ', 'VIRGINIA': 'VA', 'OHIO': 'OH', 'IOWA': 'IA', 'KENTUCKY': 'KY', 'WASHINGTON DC': 'DC', 'ARKANSAS': 'AR', 'OKLAHOMA': 'OK', 'OREGON': 'OR', 'MAINE': 'ME', 'IDAHO': 'ID', 'MINNESOTA': 'MN', 'ARIZONA': 'AZ', 'INDIANA': 'IN', 'CONNECTICUT': 'CT', 'WEST VIRGINIA': 'WV', 'NEBRASKA': 'NE', 'TENNESSEE': 'TN', 'LOUISIANA': 'LA', 'ALASKA': 'AK', 'NORTH DAKOTA': 'ND', 'MONTANA': 'MT', 'SOUTH CAROLINA': 'SC', 'NEW MEXICO': 'NM', 'RHODE ISLAND': 'RI', 'SOUTH DAKOTA': 'SD', 'WYOMING': 'WY'}\n",
      "Missing state codes before manual fix: 10\n",
      "States with missing codes: ['NAPOLI']\n",
      "Missing state codes after manual fix: 0\n",
      "Empty DataFrame\n",
      "Columns: [State, State Code]\n",
      "Index: []\n",
      "File saved to /Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with error handling for Unicode\n",
    "with open('/Users/shadivaz/Desktop/PROJECT 2/Customers.csv', encoding='utf-8', errors='replace') as file:\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "# Standardize state names to uppercase and strip whitespace\n",
    "df['State'] = df['State'].str.upper().str.strip()\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Display the state code dictionary for debugging\n",
    "print(\"State Code Dictionary:\", state_code_dict)\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'] = df['State Code'].fillna(df['State'].map(state_code_dict))\n",
    "\n",
    "# Check how many values are still missing\n",
    "missing_count_before = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes before manual fix: {missing_count_before}\")\n",
    "\n",
    "# Print the states that are not mapped\n",
    "unmapped_states = df[df['State Code'].isnull()]['State'].unique()\n",
    "print(\"States with missing codes:\", unmapped_states)\n",
    "\n",
    "# Manually set the state code for \"NAPOLI\" if still missing\n",
    "df.loc[df['State'] == 'NAPOLI', 'State Code'] = df.loc[df['State'] == 'NAPOLI', 'State Code'].fillna('NA')\n",
    "\n",
    "# Manual mapping for any known unmapped states\n",
    "# Example:\n",
    "# df.loc[df['State'] == 'UNKNOWN_STATE', 'State Code'] = 'XX'\n",
    "\n",
    "# Check how many values are still missing after the manual fix\n",
    "missing_count_after = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes after manual fix: {missing_count_after}\")\n",
    "\n",
    "# Display the DataFrame rows where state code is missing\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "print(missing_state_codes[['State', 'State Code']])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV\n",
    "output_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "54d1c167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dd5745da",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'errors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/shadivaz/Desktop/PROJECT 2/Customers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Standardize state names to uppercase and strip whitespace\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'errors'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customers.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8', errors='replace')\n",
    "\n",
    "# Standardize state names to uppercase and strip whitespace\n",
    "df['State'] = df['State'].str.upper().str.strip()\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'] = df['State Code'].fillna(df['State'].map(state_code_dict))\n",
    "\n",
    "# Check how many values are still missing\n",
    "missing_count_before = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes before manual fix: {missing_count_before}\")\n",
    "\n",
    "# Identify the states that are not getting mapped\n",
    "unmapped_states = df[df['State Code'].isnull()]['State'].unique()\n",
    "print(\"States with missing codes:\", unmapped_states)\n",
    "\n",
    "# Manual mapping for states not covered\n",
    "# Expand the dictionary with manual mappings as needed\n",
    "manual_mappings = {\n",
    "    'NAPOLI': 'NA',  # Example manual entry\n",
    "    # Add other mappings as needed\n",
    "}\n",
    "\n",
    "# Apply manual mappings\n",
    "for state, code in manual_mappings.items():\n",
    "    df.loc[df['State'] == state, 'State Code'] = df.loc[df['State'] == state, 'State Code'].fillna(code)\n",
    "\n",
    "# Check how many values are still missing after the manual fix\n",
    "missing_count_after = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes after manual fix: {missing_count_after}\")\n",
    "\n",
    "# Display the DataFrame rows where state code is still missing\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "print(missing_state_codes[['State', 'State Code']])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV\n",
    "output_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0676137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing state codes before manual fix: 10\n",
      "States with missing codes: ['NAPOLI']\n",
      "Missing state codes after manual fix: 0\n",
      "Empty DataFrame\n",
      "Columns: [State, State Code]\n",
      "Index: []\n",
      "File saved to /Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customers.csv'\n",
    "\n",
    "# Open the file with the 'replace' option for encoding errors\n",
    "with open(file_path, encoding='utf-8', errors='replace') as file:\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "# Standardize state names to uppercase and strip whitespace\n",
    "df['State'] = df['State'].str.upper().str.strip()\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'] = df['State Code'].fillna(df['State'].map(state_code_dict))\n",
    "\n",
    "# Check how many values are still missing\n",
    "missing_count_before = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes before manual fix: {missing_count_before}\")\n",
    "\n",
    "# Identify the states that are not getting mapped\n",
    "unmapped_states = df[df['State Code'].isnull()]['State'].unique()\n",
    "print(\"States with missing codes:\", unmapped_states)\n",
    "\n",
    "# Manual mapping for states not covered\n",
    "# Expand the dictionary with manual mappings as needed\n",
    "manual_mappings = {\n",
    "    'NAPOLI': 'NA',  # Example manual entry\n",
    "    # Add other mappings as needed\n",
    "}\n",
    "\n",
    "# Apply manual mappings\n",
    "for state, code in manual_mappings.items():\n",
    "    df.loc[df['State'] == state, 'State Code'] = df.loc[df['State'] == state, 'State Code'].fillna(code)\n",
    "\n",
    "# Check how many values are still missing after the manual fix\n",
    "missing_count_after = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes after manual fix: {missing_count_after}\")\n",
    "\n",
    "# Display the DataFrame rows where state code is still missing\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "print(missing_state_codes[['State', 'State Code']])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV\n",
    "output_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f8ea6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15266 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fe35772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing state codes: 0\n",
      "Empty DataFrame\n",
      "Columns: [State, State Code]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check how many state codes are missing\n",
    "missing_count = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes: {missing_count}\")\n",
    "\n",
    "# Display rows where state code is missing\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "print(missing_state_codes[['State', 'State Code']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1849ebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia    7/3/1939  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia   9/27/1979  \n",
      "2           VICTORIA     3380  Australia  Australia   5/26/1947  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia   9/17/1957  \n",
      "4           VICTORIA     3698  Australia  Australia  11/19/1965  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to verify the data has loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f51325da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing state codes before manual fix: 10\n",
      "States with missing codes: ['NAPOLI']\n",
      "Missing state codes after manual fix: 0\n",
      "Empty DataFrame\n",
      "Columns: [State, State Code]\n",
      "Index: []\n",
      "File saved to /Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia    7/3/1939  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia   9/27/1979  \n",
      "2           VICTORIA     3380  Australia  Australia   5/26/1947  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia   9/17/1957  \n",
      "4           VICTORIA     3698  Australia  Australia  11/19/1965  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customers.csv'\n",
    "\n",
    "# Open the file with the 'replace' option for encoding errors\n",
    "with open(file_path, encoding='utf-8', errors='replace') as file:\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "# Standardize state names to uppercase and strip whitespace\n",
    "df['State'] = df['State'].str.upper().str.strip()\n",
    "\n",
    "# Create a dictionary to map state names to state codes\n",
    "state_code_dict = df.dropna(subset=['State Code']).set_index('State')['State Code'].to_dict()\n",
    "\n",
    "# Fill in the missing state codes using the dictionary\n",
    "df['State Code'] = df['State Code'].fillna(df['State'].map(state_code_dict))\n",
    "\n",
    "# Check how many values are still missing\n",
    "missing_count_before = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes before manual fix: {missing_count_before}\")\n",
    "\n",
    "# Identify the states that are not getting mapped\n",
    "unmapped_states = df[df['State Code'].isnull()]['State'].unique()\n",
    "print(\"States with missing codes:\", unmapped_states)\n",
    "\n",
    "# Manual mapping for states not covered\n",
    "# Expand the dictionary with manual mappings as needed\n",
    "manual_mappings = {\n",
    "    'NAPOLI': 'NA',  # Example manual entry\n",
    "    # Add other mappings as needed\n",
    "}\n",
    "\n",
    "# Apply manual mappings\n",
    "for state, code in manual_mappings.items():\n",
    "    df.loc[df['State'] == state, 'State Code'] = df.loc[df['State'] == state, 'State Code'].fillna(code)\n",
    "\n",
    "# Check how many values are still missing after the manual fix\n",
    "missing_count_after = df['State Code'].isnull().sum()\n",
    "print(f\"Missing state codes after manual fix: {missing_count_after}\")\n",
    "\n",
    "# Display the DataFrame rows where state code is still missing\n",
    "missing_state_codes = df[df['State Code'].isnull()]\n",
    "print(missing_state_codes[['State', 'State Code']])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV\n",
    "output_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to {output_file_path}\")\n",
    "\n",
    "# Load the newly saved CSV file to verify\n",
    "updated_df = pd.read_csv(output_file_path)\n",
    "\n",
    "# Print the information of the updated DataFrame\n",
    "print(updated_df.info())\n",
    "\n",
    "# Display a few rows to verify the data\n",
    "print(updated_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "92416ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [CustomerKey, Gender, Name, City, State Code, State, Zip Code, Country, Continent, Birthday]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "missing_state_codes = df[df['State Code'].isna()]\n",
    "print(missing_state_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7ff31944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "15266\n"
     ]
    }
   ],
   "source": [
    "# Check the total number of entries and non-null entries for State Code\n",
    "print(df['State Code'].isna().sum())\n",
    "print(df['State Code'].notna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dea2bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3578ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15256\n"
     ]
    }
   ],
   "source": [
    "print(df['State Code'].notna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5fbe15c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia    7/3/1939  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia   9/27/1979  \n",
      "2           VICTORIA     3380  Australia  Australia   5/26/1947  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia   9/17/1957  \n",
      "4           VICTORIA     3698  Australia  Australia  11/19/1965  \n",
      "Total entries: 15266\n",
      "Non-null 'State Code': 15256\n"
     ]
    }
   ],
   "source": [
    "# Reload the DataFrame\n",
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv')\n",
    "\n",
    "# Confirm the reloaded DataFrame\n",
    "print(df.head())\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(f\"Non-null 'State Code': {df['State Code'].notna().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "05fae78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CustomerKey  Gender                Name               City State Code  \\\n",
      "5304       729681  Female    Rossana Padovesi            Polvica        NaN   \n",
      "5316       732289    Male        Indro Piccio          Varcaturo        NaN   \n",
      "5372       742042    Male     Amaranto Loggia          Casaferro        NaN   \n",
      "5377       742886  Female       Edmonda Capon           Terzigno        NaN   \n",
      "5378       743343  Female        Ambra Sagese  Pomigliano D'Arco        NaN   \n",
      "5485       759705    Male    Callisto Lo Duca            Casilli        NaN   \n",
      "5525       765589    Male  Michelino Lucchesi       Pompei Scavi        NaN   \n",
      "5531       766410    Male   Adelmio Beneventi             Licola        NaN   \n",
      "5631       781667  Female          Ilda Manna             Napoli        NaN   \n",
      "5695       789177    Male   Calogero Folliero      Mariglianella        NaN   \n",
      "\n",
      "       State Zip Code Country Continent    Birthday  \n",
      "5304  NAPOLI    80035   Italy    Europe   4/18/1981  \n",
      "5316  NAPOLI    80014   Italy    Europe   2/24/1949  \n",
      "5372  NAPOLI    80034   Italy    Europe   3/14/1936  \n",
      "5377  NAPOLI    80040   Italy    Europe    8/6/1963  \n",
      "5378  NAPOLI    80038   Italy    Europe    1/5/1961  \n",
      "5485  NAPOLI    80047   Italy    Europe   8/28/1976  \n",
      "5525  NAPOLI    80045   Italy    Europe  11/13/1947  \n",
      "5531  NAPOLI    80078   Italy    Europe   1/13/1940  \n",
      "5631  NAPOLI    80134   Italy    Europe    5/8/1977  \n",
      "5695  NAPOLI    80030   Italy    Europe    3/3/2000  \n"
     ]
    }
   ],
   "source": [
    "# Identify entries where 'State Code' is missing\n",
    "missing_state_codes = df[df['State Code'].isna()]\n",
    "print(missing_state_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7bd80f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Example: If you know the 'State' should have a specific 'State Code'\n",
    "df.loc[df['State'] == 'NAPOLI', 'State Code'] = 'NA'  # Replace 'NAPOLI' and 'NA' with the actual values\n",
    "\n",
    "# Recheck the missing values after the update\n",
    "print(df['State Code'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ae77beb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Recheck the DataFrame for any remaining missing values\n",
    "print(df['State Code'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "016554d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1846252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correctly reading the CSV file without specifying index\n",
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv')\n",
    "\n",
    "# Display DataFrame information\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a397019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CustomerKey  Gender                Name               City State Code  \\\n",
      "5304       729681  Female    Rossana Padovesi            Polvica        NaN   \n",
      "5316       732289    Male        Indro Piccio          Varcaturo        NaN   \n",
      "5372       742042    Male     Amaranto Loggia          Casaferro        NaN   \n",
      "5377       742886  Female       Edmonda Capon           Terzigno        NaN   \n",
      "5378       743343  Female        Ambra Sagese  Pomigliano D'Arco        NaN   \n",
      "5485       759705    Male    Callisto Lo Duca            Casilli        NaN   \n",
      "5525       765589    Male  Michelino Lucchesi       Pompei Scavi        NaN   \n",
      "5531       766410    Male   Adelmio Beneventi             Licola        NaN   \n",
      "5631       781667  Female          Ilda Manna             Napoli        NaN   \n",
      "5695       789177    Male   Calogero Folliero      Mariglianella        NaN   \n",
      "\n",
      "       State Zip Code Country Continent    Birthday  \n",
      "5304  NAPOLI    80035   Italy    Europe   4/18/1981  \n",
      "5316  NAPOLI    80014   Italy    Europe   2/24/1949  \n",
      "5372  NAPOLI    80034   Italy    Europe   3/14/1936  \n",
      "5377  NAPOLI    80040   Italy    Europe    8/6/1963  \n",
      "5378  NAPOLI    80038   Italy    Europe    1/5/1961  \n",
      "5485  NAPOLI    80047   Italy    Europe   8/28/1976  \n",
      "5525  NAPOLI    80045   Italy    Europe  11/13/1947  \n",
      "5531  NAPOLI    80078   Italy    Europe   1/13/1940  \n",
      "5631  NAPOLI    80134   Italy    Europe    5/8/1977  \n",
      "5695  NAPOLI    80030   Italy    Europe    3/3/2000  \n"
     ]
    }
   ],
   "source": [
    "# Identify entries where 'State Code' is missing\n",
    "missing_state_codes = df[df['State Code'].isna()]\n",
    "print(missing_state_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dabdf0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv')\n",
    "\n",
    "# Create a mapping dictionary for state codes\n",
    "state_code_mapping = {\n",
    "    'NAPOLI': 'NA',\n",
    "    # Include other mappings if necessary\n",
    "}\n",
    "\n",
    "# Function to fill missing 'State Code' based on 'State'\n",
    "def fill_state_code(row):\n",
    "    if pd.isna(row['State Code']):\n",
    "        return state_code_mapping.get(row['State'], row['State Code'])\n",
    "    return row['State Code']\n",
    "\n",
    "# Apply the function to update the DataFrame\n",
    "df['State Code'] = df.apply(fill_state_code, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7979c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia    7/3/1939  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia   9/27/1979  \n",
      "2           VICTORIA     3380  Australia  Australia   5/26/1947  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia   9/17/1957  \n",
      "4           VICTORIA     3698  Australia  Australia  11/19/1965  \n",
      "Number of missing 'State Code' values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows to confirm updates\n",
    "print(df.head())\n",
    "\n",
    "# Verify no missing values in 'State Code'\n",
    "missing_state_codes_count = df['State Code'].isna().sum()\n",
    "print(f\"Number of missing 'State Code' values: {missing_state_codes_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b4d63836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame back to the CSV file\n",
    "df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dd258330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia    7/3/1939  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia   9/27/1979  \n",
      "2           VICTORIA     3380  Australia  Australia   5/26/1947  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia   9/17/1957  \n",
      "4           VICTORIA     3698  Australia  Australia  11/19/1965  \n",
      "Number of missing 'State Code' values in reloaded DataFrame: 10\n"
     ]
    }
   ],
   "source": [
    "# Reload the CSV file to check if the changes are saved\n",
    "df_reloaded = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv')\n",
    "\n",
    "# Verify the changes\n",
    "print(df_reloaded.head())\n",
    "print(f\"Number of missing 'State Code' values in reloaded DataFrame: {df_reloaded['State Code'].isna().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1e5a0ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia    7/3/1939  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia   9/27/1979  \n",
      "2           VICTORIA     3380  Australia  Australia   5/26/1947  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia   9/17/1957  \n",
      "4           VICTORIA     3698  Australia  Australia  11/19/1965  \n",
      "Number of missing 'State Code' values in reloaded DataFrame: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame from the original CSV file\n",
    "df = pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv')\n",
    "\n",
    "# Mapping dictionary to fill missing 'State Code' values based on 'State'\n",
    "state_code_mapping = {\n",
    "    'NAPOLI': 'NA',\n",
    "    # Add other state mappings if required\n",
    "}\n",
    "\n",
    "# Function to fill missing 'State Code' based on 'State' column\n",
    "def fill_state_code(row):\n",
    "    if pd.isna(row['State Code']):\n",
    "        return state_code_mapping.get(row['State'], row['State Code'])\n",
    "    return row['State Code']\n",
    "\n",
    "# Apply the function to fill missing 'State Code' values\n",
    "df['State Code'] = df.apply(fill_state_code, axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "new_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv'\n",
    "df.to_csv(new_file_path, index=False)\n",
    "\n",
    "# Reload the DataFrame from the new CSV file\n",
    "df_reloaded = pd.read_csv(new_file_path)\n",
    "\n",
    "# Print the reloaded DataFrame to verify changes\n",
    "print(df_reloaded.head())\n",
    "\n",
    "# Check if there are still any missing 'State Code' values in the reloaded DataFrame\n",
    "missing_state_codes_count_reloaded = df_reloaded['State Code'].isna().sum()\n",
    "print(f\"Number of missing 'State Code' values in reloaded DataFrame: {missing_state_codes_count_reloaded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "150f42c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [CustomerKey, Gender, Name, City, State Code, State, Zip Code, Country, Continent, Birthday]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df[df['State Code'].isna()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2acbebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing state codes before fixing:\n",
      "      CustomerKey  Gender                Name               City State Code  \\\n",
      "5304       729681  Female    Rossana Padovesi            Polvica        NaN   \n",
      "5316       732289    Male        Indro Piccio          Varcaturo        NaN   \n",
      "5372       742042    Male     Amaranto Loggia          Casaferro        NaN   \n",
      "5377       742886  Female       Edmonda Capon           Terzigno        NaN   \n",
      "5378       743343  Female        Ambra Sagese  Pomigliano D'Arco        NaN   \n",
      "5485       759705    Male    Callisto Lo Duca            Casilli        NaN   \n",
      "5525       765589    Male  Michelino Lucchesi       Pompei Scavi        NaN   \n",
      "5531       766410    Male   Adelmio Beneventi             Licola        NaN   \n",
      "5631       781667  Female          Ilda Manna             Napoli        NaN   \n",
      "5695       789177    Male   Calogero Folliero      Mariglianella        NaN   \n",
      "\n",
      "       State Zip Code Country Continent    Birthday  \n",
      "5304  NAPOLI    80035   Italy    Europe   4/18/1981  \n",
      "5316  NAPOLI    80014   Italy    Europe   2/24/1949  \n",
      "5372  NAPOLI    80034   Italy    Europe   3/14/1936  \n",
      "5377  NAPOLI    80040   Italy    Europe    8/6/1963  \n",
      "5378  NAPOLI    80038   Italy    Europe    1/5/1961  \n",
      "5485  NAPOLI    80047   Italy    Europe   8/28/1976  \n",
      "5525  NAPOLI    80045   Italy    Europe  11/13/1947  \n",
      "5531  NAPOLI    80078   Italy    Europe   1/13/1940  \n",
      "5631  NAPOLI    80134   Italy    Europe    5/8/1977  \n",
      "5695  NAPOLI    80030   Italy    Europe    3/3/2000  \n",
      "Missing state codes after fixing:\n",
      "Empty DataFrame\n",
      "Columns: [CustomerKey, Gender, Name, City, State Code, State, Zip Code, Country, Continent, Birthday]\n",
      "Index: []\n",
      "Number of missing 'State Code' values in reloaded DataFrame: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV\n",
    "file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Check and print missing values\n",
    "missing_state_codes = df[df['State Code'].isna()]\n",
    "print(\"Missing state codes before fixing:\")\n",
    "print(missing_state_codes)\n",
    "\n",
    "# Fix missing state codes\n",
    "df['State Code'] = df['State Code'].fillna('NAPOLI')\n",
    "\n",
    "# Check and print after fixing\n",
    "missing_state_codes_after = df[df['State Code'].isna()]\n",
    "print(\"Missing state codes after fixing:\")\n",
    "print(missing_state_codes_after)\n",
    "\n",
    "# Save to a new CSV\n",
    "new_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv'\n",
    "df.to_csv(new_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Reload to verify\n",
    "df_reloaded = pd.read_csv(new_file_path, encoding='utf-8')\n",
    "print(\"Number of missing 'State Code' values in reloaded DataFrame:\", df_reloaded['State Code'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6441523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15266 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0e838f64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "\n",
      "Initial DataFrame Preview:\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia    7/3/1939  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia   9/27/1979  \n",
      "2           VICTORIA     3380  Australia  Australia   5/26/1947  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia   9/17/1957  \n",
      "4           VICTORIA     3698  Australia  Australia  11/19/1965  \n",
      "\n",
      "Missing 'State Code' Entries:\n",
      "      CustomerKey  Gender                Name               City State Code  \\\n",
      "5304       729681  Female    Rossana Padovesi            Polvica       None   \n",
      "5316       732289    Male        Indro Piccio          Varcaturo       None   \n",
      "5372       742042    Male     Amaranto Loggia          Casaferro       None   \n",
      "5377       742886  Female       Edmonda Capon           Terzigno       None   \n",
      "5378       743343  Female        Ambra Sagese  Pomigliano D'Arco       None   \n",
      "5485       759705    Male    Callisto Lo Duca            Casilli       None   \n",
      "5525       765589    Male  Michelino Lucchesi       Pompei Scavi       None   \n",
      "5531       766410    Male   Adelmio Beneventi             Licola       None   \n",
      "5631       781667  Female          Ilda Manna             Napoli       None   \n",
      "5695       789177    Male   Calogero Folliero      Mariglianella       None   \n",
      "\n",
      "       State Zip Code Country Continent    Birthday  \n",
      "5304  NAPOLI    80035   Italy    Europe   4/18/1981  \n",
      "5316  NAPOLI    80014   Italy    Europe   2/24/1949  \n",
      "5372  NAPOLI    80034   Italy    Europe   3/14/1936  \n",
      "5377  NAPOLI    80040   Italy    Europe    8/6/1963  \n",
      "5378  NAPOLI    80038   Italy    Europe    1/5/1961  \n",
      "5485  NAPOLI    80047   Italy    Europe   8/28/1976  \n",
      "5525  NAPOLI    80045   Italy    Europe  11/13/1947  \n",
      "5531  NAPOLI    80078   Italy    Europe   1/13/1940  \n",
      "5631  NAPOLI    80134   Italy    Europe    5/8/1977  \n",
      "5695  NAPOLI    80030   Italy    Europe    3/3/2000  \n",
      "\n",
      "Final DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15266 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "\n",
      "Final DataFrame Preview:\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia    7/3/1939  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia   9/27/1979  \n",
      "2           VICTORIA     3380  Australia  Australia   5/26/1947  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia   9/17/1957  \n",
      "4           VICTORIA     3698  Australia  Australia  11/19/1965  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated1.csv'\n",
    "output_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Customers_corrected.csv'\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(input_file_path, encoding='utf-8')\n",
    "\n",
    "# Print initial DataFrame info\n",
    "print(\"Initial DataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Print first few rows to inspect\n",
    "print(\"\\nInitial DataFrame Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Define the mapping of state names to state codes\n",
    "# You need to fill this dictionary with actual values\n",
    "state_code_mapping = {\n",
    "    'SOUTH AUSTRALIA': 'SA',\n",
    "    'WESTERN AUSTRALIA': 'WA',\n",
    "    'VICTORIA': 'VIC',\n",
    "    'NEW SOUTH WALES': 'NSW',\n",
    "    # Add more states and their codes as needed\n",
    "    'NAPOLI': None  # Specify if 'NAPOLI' should be replaced with NaN or specific state code\n",
    "}\n",
    "\n",
    "# Replace missing 'State Code' values based on 'State' column\n",
    "df['State Code'] = df.apply(\n",
    "    lambda row: state_code_mapping.get(row['State'], pd.NA) if pd.isna(row['State Code']) else row['State Code'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Fill specific placeholder values if needed\n",
    "df['State Code'].replace('NAPOLI', pd.NA, inplace=True)\n",
    "\n",
    "# Verify missing values\n",
    "missing_state_codes = df[df['State Code'].isna()]\n",
    "print(\"\\nMissing 'State Code' Entries:\")\n",
    "print(missing_state_codes)\n",
    "\n",
    "# Fill remaining NaN values in 'State Code' if necessary\n",
    "# For example, using a placeholder 'UNKNOWN' or other logic\n",
    "df['State Code'].fillna('UNKNOWN', inplace=True)\n",
    "\n",
    "# Save the corrected DataFrame to a new CSV file\n",
    "df.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Print final DataFrame info and preview\n",
    "print(\"\\nFinal DataFrame Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFinal DataFrame Preview:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e22da184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15266 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "915f9b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Field</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sales</td>\n",
       "      <td>Order Number</td>\n",
       "      <td>Unique ID for each order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sales</td>\n",
       "      <td>Line Item</td>\n",
       "      <td>Identifies individual products purchased as pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sales</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Date the order was placed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sales</td>\n",
       "      <td>Delivery Date</td>\n",
       "      <td>Date the order was delivered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sales</td>\n",
       "      <td>CustomerKey</td>\n",
       "      <td>Unique key identifying which customer placed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sales</td>\n",
       "      <td>StoreKey</td>\n",
       "      <td>Unique key identifying which store processed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sales</td>\n",
       "      <td>ProductKey</td>\n",
       "      <td>Unique key identifying which product was purch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sales</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>Number of items purchased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sales</td>\n",
       "      <td>Currency Code</td>\n",
       "      <td>Currency used to process the order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Customers</td>\n",
       "      <td>CustomerKey</td>\n",
       "      <td>Primary key to identify customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Customers</td>\n",
       "      <td>Gender</td>\n",
       "      <td>Customer gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Customers</td>\n",
       "      <td>Name</td>\n",
       "      <td>Customer full name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Customers</td>\n",
       "      <td>City</td>\n",
       "      <td>Customer city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Customers</td>\n",
       "      <td>State Code</td>\n",
       "      <td>Customer state (abbreviated)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Customers</td>\n",
       "      <td>State</td>\n",
       "      <td>Customer state (full)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Customers</td>\n",
       "      <td>Zip Code</td>\n",
       "      <td>Customer zip code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Customers</td>\n",
       "      <td>Country</td>\n",
       "      <td>Customer country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Customers</td>\n",
       "      <td>Continent</td>\n",
       "      <td>Customer continent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Customers</td>\n",
       "      <td>Birthday</td>\n",
       "      <td>Customer date of birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Products</td>\n",
       "      <td>ProductKey</td>\n",
       "      <td>Primary key to identify products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Products</td>\n",
       "      <td>Product Name</td>\n",
       "      <td>Product name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Products</td>\n",
       "      <td>Brand</td>\n",
       "      <td>Product brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Products</td>\n",
       "      <td>Color</td>\n",
       "      <td>Product color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Products</td>\n",
       "      <td>Unit Cost USD</td>\n",
       "      <td>Cost to produce the product in USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Products</td>\n",
       "      <td>Unit Price USD</td>\n",
       "      <td>Product list price in USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Products</td>\n",
       "      <td>SubcategoryKey</td>\n",
       "      <td>Key to identify product subcategories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Products</td>\n",
       "      <td>Subcategory</td>\n",
       "      <td>Product subcategory name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Products</td>\n",
       "      <td>CategoryKey</td>\n",
       "      <td>Key to identify product categories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Products</td>\n",
       "      <td>Category</td>\n",
       "      <td>Product category name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Stores</td>\n",
       "      <td>StoreKey</td>\n",
       "      <td>Primary key to identify stores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Stores</td>\n",
       "      <td>Country</td>\n",
       "      <td>Store country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Stores</td>\n",
       "      <td>State</td>\n",
       "      <td>Store state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Stores</td>\n",
       "      <td>Square Meters</td>\n",
       "      <td>Store footprint in square meters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Stores</td>\n",
       "      <td>Open Date</td>\n",
       "      <td>Store open date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Exchange Rates</td>\n",
       "      <td>Date</td>\n",
       "      <td>Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Exchange Rates</td>\n",
       "      <td>Currency</td>\n",
       "      <td>Currency code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Exchange Rates</td>\n",
       "      <td>Exchange</td>\n",
       "      <td>Exchange rate compared to USD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Table           Field  \\\n",
       "0            Sales    Order Number   \n",
       "1            Sales       Line Item   \n",
       "2            Sales      Order Date   \n",
       "3            Sales   Delivery Date   \n",
       "4            Sales     CustomerKey   \n",
       "5            Sales        StoreKey   \n",
       "6            Sales      ProductKey   \n",
       "7            Sales        Quantity   \n",
       "8            Sales   Currency Code   \n",
       "9        Customers     CustomerKey   \n",
       "10       Customers          Gender   \n",
       "11       Customers            Name   \n",
       "12       Customers            City   \n",
       "13       Customers      State Code   \n",
       "14       Customers           State   \n",
       "15       Customers        Zip Code   \n",
       "16       Customers         Country   \n",
       "17       Customers       Continent   \n",
       "18       Customers        Birthday   \n",
       "19        Products      ProductKey   \n",
       "20        Products    Product Name   \n",
       "21        Products           Brand   \n",
       "22        Products           Color   \n",
       "23        Products   Unit Cost USD   \n",
       "24        Products  Unit Price USD   \n",
       "25        Products  SubcategoryKey   \n",
       "26        Products     Subcategory   \n",
       "27        Products     CategoryKey   \n",
       "28        Products        Category   \n",
       "29          Stores        StoreKey   \n",
       "30          Stores         Country   \n",
       "31          Stores           State   \n",
       "32          Stores   Square Meters   \n",
       "33          Stores       Open Date   \n",
       "34  Exchange Rates            Date   \n",
       "35  Exchange Rates        Currency   \n",
       "36  Exchange Rates        Exchange   \n",
       "\n",
       "                                          Description  \n",
       "0                            Unique ID for each order  \n",
       "1   Identifies individual products purchased as pa...  \n",
       "2                           Date the order was placed  \n",
       "3                        Date the order was delivered  \n",
       "4   Unique key identifying which customer placed t...  \n",
       "5   Unique key identifying which store processed t...  \n",
       "6   Unique key identifying which product was purch...  \n",
       "7                           Number of items purchased  \n",
       "8                  Currency used to process the order  \n",
       "9                   Primary key to identify customers  \n",
       "10                                    Customer gender  \n",
       "11                                 Customer full name  \n",
       "12                                      Customer city  \n",
       "13                       Customer state (abbreviated)  \n",
       "14                              Customer state (full)  \n",
       "15                                  Customer zip code  \n",
       "16                                   Customer country  \n",
       "17                                 Customer continent  \n",
       "18                             Customer date of birth  \n",
       "19                   Primary key to identify products  \n",
       "20                                       Product name  \n",
       "21                                      Product brand  \n",
       "22                                      Product color  \n",
       "23                 Cost to produce the product in USD  \n",
       "24                          Product list price in USD  \n",
       "25              Key to identify product subcategories  \n",
       "26                           Product subcategory name  \n",
       "27                 Key to identify product categories  \n",
       "28                              Product category name  \n",
       "29                     Primary key to identify stores  \n",
       "30                                      Store country  \n",
       "31                                        Store state  \n",
       "32                   Store footprint in square meters  \n",
       "33                                    Store open date  \n",
       "34                                               Date  \n",
       "35                                      Currency code  \n",
       "36                      Exchange rate compared to USD  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"/Users/shadivaz/Desktop/PROJECT 2/Data_Dictionary.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b577b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: sqlalchemy in /opt/anaconda3/lib/python3.11/site-packages (2.0.25)\n",
      "Requirement already satisfied: mysql-connector-python in /opt/anaconda3/lib/python3.11/site-packages (8.4.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from sqlalchemy) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas sqlalchemy mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c945d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'project2' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# MySQL server connection details\n",
    "mysql_config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': 'Shadi8903!'\n",
    "}\n",
    "\n",
    "# Connect to the MySQL server (not to a specific database)\n",
    "connection = mysql.connector.connect(\n",
    "    host=mysql_config['host'],\n",
    "    user=mysql_config['user'],\n",
    "    password=mysql_config['password']\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Create a new database\n",
    "database_name = \"project2\"\n",
    "cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "print(f\"Database '{database_name}' created successfully.\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d5d8277c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Insert into the database\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df\u001b[38;5;241m.\u001b[39mto_sql(name\u001b[38;5;241m=\u001b[39mtable_name, con\u001b[38;5;241m=\u001b[39mengine, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inserted into the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m table.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'engine' is not defined"
     ]
    }
   ],
   "source": [
    "# Paths to your CSV files\n",
    "csv_files = {\n",
    "    'Customers': '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv',\n",
    "    'Sales': '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv',\n",
    "    'Products': '/Users/shadivaz/Desktop/PROJECT 2/Products.csv',\n",
    "    'Stores': '/Users/shadivaz/Desktop/PROJECT 2/Stores.csv',\n",
    "    'ExchangeRates': '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv'\n",
    "}\n",
    "\n",
    "# Load and insert each CSV into the database\n",
    "for table_name, file_path in csv_files.items():\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Insert into the database\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "    \n",
    "    print(f\"Data from {file_path} inserted into the {table_name} table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ba3b9501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from /Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv inserted into the Customers table.\n",
      "Data from /Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv inserted into the Sales table.\n",
      "Data from /Users/shadivaz/Desktop/PROJECT 2/Products.csv inserted into the Products table.\n",
      "Data from /Users/shadivaz/Desktop/PROJECT 2/Stores.csv inserted into the Stores table.\n",
      "Data from /Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv inserted into the ExchangeRates table.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL server connection details\n",
    "mysql_config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': 'Shadi8903!'\n",
    "}\n",
    "\n",
    "# Create the SQLAlchemy engine for the existing database\n",
    "database_name = \"project2\"\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{mysql_config['user']}:{mysql_config['password']}@{mysql_config['host']}/{database_name}\")\n",
    "\n",
    "# Paths to your CSV files\n",
    "csv_files = {\n",
    "    'Customers': '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv',\n",
    "    'Sales': '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv',\n",
    "    'Products': '/Users/shadivaz/Desktop/PROJECT 2/Products.csv',\n",
    "    'Stores': '/Users/shadivaz/Desktop/PROJECT 2/Stores.csv',\n",
    "    'ExchangeRates': '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv'\n",
    "}\n",
    "\n",
    "# Load and insert each CSV into the database\n",
    "for table_name, file_path in csv_files.items():\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Insert into the database\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    print(f\"Data from {file_path} inserted into the {table_name} table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c71bebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column headings for Customers: ['CustomerKey', 'Gender', 'Name', 'City', 'State Code', 'State', 'Zip Code', 'Country', 'Continent', 'Birthday']\n",
      "Column headings for Sales: ['Order Number', 'Line Item', 'Order Date', 'Delivery Date', 'CustomerKey', 'StoreKey', 'ProductKey', 'Quantity', 'Currency Code']\n",
      "Column headings for Products: ['ProductKey', 'Product Name', 'Brand', 'Color', 'Unit Cost USD', 'Unit Price USD', 'SubcategoryKey', 'Subcategory', 'CategoryKey', 'Category']\n",
      "Column headings for Stores: ['StoreKey', 'Country', 'State', 'Square Meters', 'Open Date']\n",
      "Column headings for ExchangeRates: ['Date', 'Currency', 'Exchange']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "csv_files = {\n",
    "    'Customers': '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv',\n",
    "    'Sales': '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv',\n",
    "    'Products': '/Users/shadivaz/Desktop/PROJECT 2/Products.csv',\n",
    "    'Stores': '/Users/shadivaz/Desktop/PROJECT 2/Stores.csv',\n",
    "    'ExchangeRates': '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv'\n",
    "}\n",
    "\n",
    "# Loop through the files and print the column headings for each\n",
    "for table_name, file_path in csv_files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    column_headings = df.columns.tolist()\n",
    "    print(f\"Column headings for {table_name}: {column_headings}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c771db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading table Customers in chunks...\n",
      "Loading table Sales in chunks...\n",
      "Loading table Products in chunks...\n",
      "Loading table Stores in chunks...\n",
      "Loading table ExchangeRates in chunks...\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n",
      "Processed a chunk of size (5000000, 34)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL server connection details\n",
    "mysql_config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': 'Shadi8903!'\n",
    "}\n",
    "\n",
    "# Create the SQLAlchemy engine for the existing database\n",
    "database_name = \"project2\"\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{mysql_config['user']}:{mysql_config['password']}@{mysql_config['host']}/{database_name}\")\n",
    "\n",
    "# Function to load data in chunks\n",
    "def load_table(table_name, engine, columns=None, chunksize=5000):\n",
    "    print(f\"Loading table {table_name} in chunks...\")\n",
    "    return pd.read_sql_table(table_name, con=engine, columns=columns, chunksize=chunksize)\n",
    "\n",
    "# Load tables with selected columns to minimize memory usage\n",
    "customers_chunks = load_table('Customers', engine, chunksize=5000)\n",
    "sales_chunks = load_table('Sales', engine, chunksize=5000)\n",
    "products_chunks = load_table('Products', engine, chunksize=5000)\n",
    "stores_chunks = load_table('Stores', engine, chunksize=5000)\n",
    "exchange_rates_chunks = load_table('ExchangeRates', engine, chunksize=5000)\n",
    "\n",
    "# Initialize the output CSV file\n",
    "output_csv_path = 'Combined_Data_Optimized.csv'\n",
    "\n",
    "# Write the header to the CSV file\n",
    "first_chunk = True\n",
    "\n",
    "# Read first chunk from each table\n",
    "customers_chunk = next(customers_chunks)\n",
    "products_chunk = next(products_chunks)\n",
    "stores_chunk = next(stores_chunks)\n",
    "exchange_rates_chunk = next(exchange_rates_chunks)\n",
    "\n",
    "# Process chunks to reduce memory usage\n",
    "for sales_chunk in sales_chunks:\n",
    "    # Merge with Customers\n",
    "    merged_chunk = sales_chunk.merge(customers_chunk, on='CustomerKey', how='left', suffixes=('', '_cust'))\n",
    "\n",
    "    # Merge with Products\n",
    "    merged_chunk = merged_chunk.merge(products_chunk, on='ProductKey', how='left', suffixes=('', '_prod'))\n",
    "\n",
    "    # Merge with Stores\n",
    "    merged_chunk = merged_chunk.merge(stores_chunk, on='StoreKey', how='left', suffixes=('', '_store'))\n",
    "\n",
    "    # Merge with ExchangeRates\n",
    "    merged_chunk = merged_chunk.merge(exchange_rates_chunk, left_on='Currency Code', right_on='Currency', how='left', suffixes=('', '_exch'))\n",
    "\n",
    "    # Write chunk to CSV file\n",
    "    merged_chunk.to_csv(output_csv_path, mode='a', header=first_chunk, index=False)\n",
    "    \n",
    "    # After writing the first chunk, set `first_chunk` to False\n",
    "    first_chunk = False\n",
    "    \n",
    "    print(f\"Processed a chunk of size {merged_chunk.shape}\")\n",
    "\n",
    "print(f\"Combined data saved to {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391b03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to /Users/shadivaz/Desktop/PROJECT 2/Cleaned_Sales.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_path = '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv'\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Delivery Date' to datetime, invalid parsing will be set as NaT\n",
    "df['Delivery Date'] = pd.to_datetime(df['Delivery Date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Optional: If you want to fill NaT with a specific placeholder, you can do it like this:\n",
    "# df['Delivery Date'].fillna(pd.Timestamp('1900-01-01'), inplace=True)\n",
    "\n",
    "# Save the cleaned data\n",
    "cleaned_file_path = '/Users/shadivaz/Desktop/PROJECT 2/Cleaned_Sales.csv'\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de21a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Customers...\n",
      "Cleaned data saved to /Users/shadivaz/Desktop/PROJECT 2/Cleaned_Customers.csv\n",
      "Cleaning Exchange_Rates...\n",
      "Cleaned data saved to /Users/shadivaz/Desktop/PROJECT 2/Cleaned_Exchange_Rates.csv\n",
      "Cleaning Products...\n",
      "Cleaned data saved to /Users/shadivaz/Desktop/PROJECT 2/Cleaned_Products.csv\n",
      "Cleaning Sales...\n",
      "Cleaned data saved to /Users/shadivaz/Desktop/PROJECT 2/Cleaned_Sales.csv\n",
      "Cleaning Stores...\n",
      "Cleaned data saved to /Users/shadivaz/Desktop/PROJECT 2/Cleaned_Stores.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_paths = {\n",
    "    'Customers': '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv',\n",
    "    'Exchange_Rates': '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv',\n",
    "    'Products': '/Users/shadivaz/Desktop/PROJECT 2/Products.csv',\n",
    "    'Sales': '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv',\n",
    "    'Stores': '/Users/shadivaz/Desktop/PROJECT 2/Stores.csv'\n",
    "}\n",
    "\n",
    "# Function to clean date columns\n",
    "def clean_date_columns(df, date_columns, date_format='%m/%d/%Y'):\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert to datetime\n",
    "            df[col] = pd.to_datetime(df[col], format=date_format, errors='coerce')\n",
    "            # Convert back to string in YYYY-MM-DD format\n",
    "            df[col] = df[col].dt.strftime('%Y-%m-%d')\n",
    "    return df\n",
    "\n",
    "# Process each file\n",
    "for table_name, file_path in file_paths.items():\n",
    "    print(f\"Cleaning {table_name}...\")\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define date columns for each file\n",
    "    date_columns = []\n",
    "    if table_name == 'Sales':\n",
    "        date_columns = ['Order Date', 'Delivery Date']\n",
    "    elif table_name == 'Stores':\n",
    "        date_columns = ['Open Date']\n",
    "    elif table_name == 'Customers':\n",
    "        date_columns = ['Birthday']\n",
    "    elif table_name == 'Exchange_Rates':\n",
    "        date_columns = ['Date']  # Add date columns for Exchange_Rates if there are others\n",
    "    \n",
    "    # Clean date columns\n",
    "    df = clean_date_columns(df, date_columns)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    cleaned_file_path = f'/Users/shadivaz/Desktop/PROJECT 2/Cleaned_{table_name}.csv'\n",
    "    df.to_csv(cleaned_file_path, index=False)\n",
    "    \n",
    "    print(f\"Cleaned data saved to {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d355f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2016-01-13\n",
      "1    2016-01-13\n",
      "2    2016-01-13\n",
      "3    2016-01-12\n",
      "4    2016-01-12\n",
      "5    2016-01-12\n",
      "6    2016-01-12\n",
      "7    2016-01-12\n",
      "8    2016-01-12\n",
      "9    2016-01-12\n",
      "Name: Delivery Date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the 'Delivery Date' column\n",
    "print(df['Delivery Date'].head(10))  # Print first 10 rows to check format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e2a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_7097/3764187163.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(column, errors='coerce', infer_datetime_format=True)\n",
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_7097/3764187163.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(column, errors='coerce', infer_datetime_format=True)\n",
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_7097/3764187163.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(column, errors='coerce', infer_datetime_format=True)\n",
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_7097/3764187163.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(column, errors='coerce', infer_datetime_format=True)\n",
      "/var/folders/js/_3k7czbj31q65fs33w_cc5740000gn/T/ipykernel_7097/3764187163.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(column, errors='coerce', infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "sales_file = '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv'\n",
    "stores_file = '/Users/shadivaz/Desktop/PROJECT 2/Stores.csv'\n",
    "exchange_rates_file = '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv'\n",
    "customers_file = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv'\n",
    "\n",
    "# Load the CSV files\n",
    "sales_df = pd.read_csv(sales_file)\n",
    "stores_df = pd.read_csv(stores_file)\n",
    "exchange_rates_df = pd.read_csv(exchange_rates_file)\n",
    "customers_df = pd.read_csv(customers_file)\n",
    "\n",
    "# Function to convert date column with multiple formats\n",
    "def convert_date(column):\n",
    "    # Attempt to parse the dates using pd.to_datetime\n",
    "    return pd.to_datetime(column, errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "# Convert date columns in Sales Data\n",
    "sales_df['Order Date'] = convert_date(sales_df['Order Date'])\n",
    "sales_df['Delivery Date'] = convert_date(sales_df['Delivery Date'])\n",
    "\n",
    "# Convert date column in Stores Data\n",
    "stores_df['Open Date'] = convert_date(stores_df['Open Date'])\n",
    "\n",
    "# Convert date column in Exchange Rates Data\n",
    "exchange_rates_df['Date'] = convert_date(exchange_rates_df['Date'])\n",
    "\n",
    "# Convert 'Birthday' column in Customers Data\n",
    "customers_df['Birthday'] = convert_date(customers_df['Birthday'])\n",
    "\n",
    "# Save the cleaned data back to CSV\n",
    "sales_df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Sales_cleaned.csv', index=False)\n",
    "stores_df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Stores_cleaned.csv', index=False)\n",
    "exchange_rates_df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates_cleaned.csv', index=False)\n",
    "customers_df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bc8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "sales_file = '/Users/shadivaz/Desktop/PROJECT 2/Sales_filled.csv'\n",
    "stores_file = '/Users/shadivaz/Desktop/PROJECT 2/Stores.csv'\n",
    "exchange_rates_file = '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates.csv'\n",
    "customers_file = '/Users/shadivaz/Desktop/PROJECT 2/Customersupdated2.csv'\n",
    "\n",
    "# Load CSV files\n",
    "sales_df = pd.read_csv(sales_file)\n",
    "stores_df = pd.read_csv(stores_file)\n",
    "exchange_rates_df = pd.read_csv(exchange_rates_file)\n",
    "customers_df = pd.read_csv(customers_file)\n",
    "\n",
    "# Function to convert date columns with known formats\n",
    "def convert_date(column, format=None):\n",
    "    if format:\n",
    "        # Try parsing with the specified format\n",
    "        return pd.to_datetime(column, format=format, errors='coerce')\n",
    "    else:\n",
    "        # General parsing if format is unknown or mixed\n",
    "        return pd.to_datetime(column, errors='coerce')\n",
    "\n",
    "# Convert date columns in Sales Data\n",
    "sales_df['Order Date'] = convert_date(sales_df['Order Date'])\n",
    "sales_df['Delivery Date'] = convert_date(sales_df['Delivery Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Convert date column in Stores Data\n",
    "stores_df['Open Date'] = convert_date(stores_df['Open Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Convert date column in Exchange Rates Data\n",
    "exchange_rates_df['Date'] = convert_date(exchange_rates_df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Convert 'Birthday' column in Customers Data\n",
    "customers_df['Birthday'] = convert_date(customers_df['Birthday'], format='%d/%m/%Y')\n",
    "\n",
    "# Save the cleaned data back to CSV\n",
    "sales_df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Sales_cleaned.csv', index=False)\n",
    "stores_df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Stores_cleaned.csv', index=False)\n",
    "exchange_rates_df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates_cleaned.csv', index=False)\n",
    "customers_df.to_csv('/Users/shadivaz/Desktop/PROJECT 2/Customers_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77cf4945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet names:\n",
      "Sheet1\n",
      "Sheet2\n",
      "Sheet3\n",
      "Sheet4\n",
      "Sheet5\n",
      "\n",
      "Columns in sheet 'Sheet1':\n",
      "['CustomerKey', 'Gender', 'Name', 'City', 'State Code', 'State', 'Zip Code', 'Country', 'Continent', 'Birthday']\n",
      "\n",
      "Columns in sheet 'Sheet2':\n",
      "['Date', 'Currency', 'Exchange']\n",
      "\n",
      "Columns in sheet 'Sheet3':\n",
      "['ProductKey', 'Product Name', 'Brand', 'Color', 'Unit Cost USD', 'Unit Price USD', 'SubcategoryKey', 'Subcategory', 'CategoryKey', 'Category']\n",
      "\n",
      "Columns in sheet 'Sheet4':\n",
      "['Order Number', 'Line Item', 'Order Date', 'Delivery Date', 'CustomerKey', 'StoreKey', 'ProductKey', 'Quantity', 'Currency Code']\n",
      "\n",
      "Columns in sheet 'Sheet5':\n",
      "['StoreKey', 'Country', 'State', 'Square Meters', 'Open Date']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path for the Excel workbook\n",
    "input_excel_file = '/Users/shadivaz/Desktop/dataspark copy.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = pd.ExcelFile(input_excel_file)\n",
    "\n",
    "# Get sheet names\n",
    "sheet_names = excel_file.sheet_names\n",
    "print(\"Sheet names:\")\n",
    "for sheet in sheet_names:\n",
    "    print(sheet)\n",
    "\n",
    "# Get column names for each sheet\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(input_excel_file, sheet_name=sheet)\n",
    "    print(f\"\\nColumns in sheet '{sheet}':\")\n",
    "    print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "604e3abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved Sheet1 to /Users/shadivaz/Desktop/PROJECT 2/Customers_cleaned.csv\n",
      "Processed and saved Sheet2 to /Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates_cleaned.csv\n",
      "Processed and saved Sheet3 to /Users/shadivaz/Desktop/PROJECT 2/Products_cleaned.csv\n",
      "Processed and saved Sheet4 to /Users/shadivaz/Desktop/PROJECT 2/Sales_cleaned.csv\n",
      "Processed and saved Sheet5 to /Users/shadivaz/Desktop/PROJECT 2/Stores_cleaned.csv\n",
      "All sheets processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path for the Excel workbook\n",
    "input_excel_file = '/Users/shadivaz/Desktop/dataspark copy.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = pd.ExcelFile(input_excel_file)\n",
    "\n",
    "# Define output CSV file paths\n",
    "output_csv_files = {\n",
    "    'Sheet1': '/Users/shadivaz/Desktop/PROJECT 2/Customers_cleaned.csv',\n",
    "    'Sheet2': '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates_cleaned.csv',\n",
    "    'Sheet3': '/Users/shadivaz/Desktop/PROJECT 2/Products_cleaned.csv',\n",
    "    'Sheet4': '/Users/shadivaz/Desktop/PROJECT 2/Sales_cleaned.csv',\n",
    "    'Sheet5': '/Users/shadivaz/Desktop/PROJECT 2/Stores_cleaned.csv'\n",
    "}\n",
    "\n",
    "# Function to clean date columns\n",
    "def clean_dates(df, date_columns, format='%m/%d/%Y'):\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], format=format, errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Process each sheet\n",
    "for sheet in excel_file.sheet_names:\n",
    "    df = pd.read_excel(input_excel_file, sheet_name=sheet)\n",
    "    \n",
    "    if sheet == 'Sheet1':  # Customers\n",
    "        df = clean_dates(df, ['Birthday'])\n",
    "    \n",
    "    elif sheet == 'Sheet2':  # Exchange Rates\n",
    "        df = clean_dates(df, ['Date'])\n",
    "    \n",
    "    elif sheet == 'Sheet3':  # Products\n",
    "        # No date cleaning needed for products\n",
    "        pass\n",
    "    \n",
    "    elif sheet == 'Sheet4':  # Sales\n",
    "        df = clean_dates(df, ['Order Date', 'Delivery Date'])\n",
    "    \n",
    "    elif sheet == 'Sheet5':  # Stores\n",
    "        df = clean_dates(df, ['Open Date'])\n",
    "    \n",
    "    # Save cleaned data to CSV\n",
    "    output_csv_file = output_csv_files[sheet]\n",
    "    df.to_csv(output_csv_file, index=False)\n",
    "    print(f\"Processed and saved {sheet} to {output_csv_file}\")\n",
    "\n",
    "print(\"All sheets processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7dcc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dates before parsing: [14429, 29125, 17313, 21080, 24065, 19744, 25529, 18334, 24097, 27600]\n",
      "Sample dates after parsing: [NaT, NaT, NaT, NaT, NaT, NaT, NaT, NaT, NaT, NaT]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path for the Excel workbook\n",
    "input_excel_file = '/Users/shadivaz/Desktop/dataspark copy.xlsx'\n",
    "\n",
    "# Load a sample sheet\n",
    "sheet_name = 'Sheet1'  # Change this to a specific sheet for testing\n",
    "df = pd.read_excel(input_excel_file, sheet_name=sheet_name)\n",
    "\n",
    "# Test date parsing\n",
    "test_dates = df['Birthday'].head(10)  # Adjust column name and number of rows as needed\n",
    "print(\"Sample dates before parsing:\", test_dates.tolist())\n",
    "\n",
    "# Apply date parsing\n",
    "df['Birthday'] = pd.to_datetime(df['Birthday'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Check results\n",
    "print(\"Sample dates after parsing:\", df['Birthday'].head(10).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58833548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved Sheet1 to /Users/shadivaz/Desktop/PROJECT 2/Customers_cleaned.csv\n",
      "Processed and saved Sheet2 to /Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates_cleaned.csv\n",
      "Processed and saved Sheet3 to /Users/shadivaz/Desktop/PROJECT 2/Products_cleaned.csv\n",
      "Processed and saved Sheet4 to /Users/shadivaz/Desktop/PROJECT 2/Sales_cleaned.csv\n",
      "Processed and saved Sheet5 to /Users/shadivaz/Desktop/PROJECT 2/Stores_cleaned.csv\n",
      "All sheets processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def excel_serial_to_date(serial_date):\n",
    "    # Excel's base date is 1899-12-30, and Excel counts days starting from this date\n",
    "    base_date = pd.Timestamp('1899-12-30')\n",
    "    return base_date + pd.to_timedelta(serial_date, unit='D')\n",
    "\n",
    "# Define file path for the Excel workbook\n",
    "input_excel_file = '/Users/shadivaz/Desktop/dataspark copy.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = pd.ExcelFile(input_excel_file)\n",
    "\n",
    "# Define output CSV file paths\n",
    "output_csv_files = {\n",
    "    'Sheet1': '/Users/shadivaz/Desktop/PROJECT 2/Customers_cleaned.csv',\n",
    "    'Sheet2': '/Users/shadivaz/Desktop/PROJECT 2/Exchange_Rates_cleaned.csv',\n",
    "    'Sheet3': '/Users/shadivaz/Desktop/PROJECT 2/Products_cleaned.csv',\n",
    "    'Sheet4': '/Users/shadivaz/Desktop/PROJECT 2/Sales_cleaned.csv',\n",
    "    'Sheet5': '/Users/shadivaz/Desktop/PROJECT 2/Stores_cleaned.csv'\n",
    "}\n",
    "\n",
    "# Function to clean date columns\n",
    "def clean_dates(df, date_columns):\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: excel_serial_to_date(x) if pd.notna(x) else pd.NaT)\n",
    "    return df\n",
    "\n",
    "# Process each sheet\n",
    "for sheet in excel_file.sheet_names:\n",
    "    df = pd.read_excel(input_excel_file, sheet_name=sheet)\n",
    "    \n",
    "    if sheet == 'Sheet1':  # Customers\n",
    "        df = clean_dates(df, ['Birthday'])\n",
    "    \n",
    "    elif sheet == 'Sheet2':  # Exchange Rates\n",
    "        df = clean_dates(df, ['Date'])\n",
    "    \n",
    "    elif sheet == 'Sheet3':  # Products\n",
    "        # No date cleaning needed for products\n",
    "        pass\n",
    "    \n",
    "    elif sheet == 'Sheet4':  # Sales\n",
    "        df = clean_dates(df, ['Order Date', 'Delivery Date'])\n",
    "    \n",
    "    elif sheet == 'Sheet5':  # Stores\n",
    "        df = clean_dates(df, ['Open Date'])\n",
    "    \n",
    "    # Save cleaned data to CSV\n",
    "    output_csv_file = output_csv_files[sheet]\n",
    "    df.to_csv(output_csv_file, index=False)\n",
    "    print(f\"Processed and saved {sheet} to {output_csv_file}\")\n",
    "\n",
    "print(\"All sheets processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1d87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = '/Users/shadivaz/Desktop/dataspark copy.xlsx'\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Define a function to parse dates from the Excel file\n",
    "def parse_dates(df, date_columns):\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], format='%m/%d/%Y', errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Process each sheet and save it\n",
    "for sheet_name in excel_data.sheet_names:\n",
    "    df = excel_data.parse(sheet_name)\n",
    "    \n",
    "    if sheet_name == 'Sheet1':\n",
    "        date_columns = ['Birthday']\n",
    "    elif sheet_name == 'Sheet2':\n",
    "        date_columns = ['Date']\n",
    "    elif sheet_name == 'Sheet3':\n",
    "        date_columns = []  # No date columns\n",
    "    elif sheet_name == 'Sheet4':\n",
    "        date_columns = ['Order Date', 'Delivery Date']\n",
    "    elif sheet_name == 'Sheet5':\n",
    "        date_columns = ['Open Date']\n",
    "    \n",
    "df = parse_dates(df, date_columns)\n",
    "    \n",
    "# Save the processed data to a new file or overwrite\n",
    "df.to_csv(f'/Users/shadivaz/Desktop/{sheet_name}_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d19b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet1 Data:\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent  Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia       NaN  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia       NaN  \n",
      "2           VICTORIA     3380  Australia  Australia       NaN  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia       NaN  \n",
      "4           VICTORIA     3698  Australia  Australia       NaN  \n",
      "Sheet2 Data:\n",
      "   Date Currency  Exchange\n",
      "0   NaN      USD    1.0000\n",
      "1   NaN      CAD    1.1583\n",
      "2   NaN      AUD    1.2214\n",
      "3   NaN      EUR    0.8237\n",
      "4   NaN      GBP    0.6415\n",
      "Sheet3 Data:\n",
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "   Unit Cost USD  Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0           6.62           12.99             101     MP4&MP3            1   \n",
      "1           6.62           12.99             101     MP4&MP3            1   \n",
      "2           7.40           14.52             101     MP4&MP3            1   \n",
      "3          11.00           21.57             101     MP4&MP3            1   \n",
      "4          11.00           21.57             101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n",
      "Sheet4 Data:\n",
      "   Order Number  Line Item  Order Date  Delivery Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1         NaN            NaN       265598        10   \n",
      "1        366001          1         NaN            NaN      1269051         0   \n",
      "2        366001          2         NaN            NaN      1269051         0   \n",
      "3        366002          1         NaN            NaN       266019         0   \n",
      "4        366002          2         NaN            NaN       266019         0   \n",
      "\n",
      "   ProductKey  Quantity Currency Code  \n",
      "0        1304         1           CAD  \n",
      "1        1048         2           USD  \n",
      "2        2007         1           USD  \n",
      "3        1106         7           CAD  \n",
      "4         373         1           CAD  \n",
      "Sheet5 Data:\n",
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0        NaN\n",
      "1         2  Australia            Northern Territory          665.0        NaN\n",
      "2         3  Australia               South Australia         2000.0        NaN\n",
      "3         4  Australia                      Tasmania         2000.0        NaN\n",
      "4         5  Australia                      Victoria         2000.0        NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if the CSV files are created and have data\n",
    "for sheet_name in ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4', 'Sheet5']:\n",
    "    file_path = f'/Users/shadivaz/Desktop/{sheet_name}_processed.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f'{sheet_name} Data:')\n",
    "        print(df.head())\n",
    "    except FileNotFoundError:\n",
    "        print(f'File {file_path} not found.')\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {file_path}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a301fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df_sheet1 = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Function to parse dates\n",
    "def parse_date(date_str):\n",
    "    if pd.notna(date_str):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%m/%d/%Y')\n",
    "        except ValueError:\n",
    "            return pd.to_datetime(date_str, errors='coerce')  # Fallback if format doesn't match\n",
    "    return pd.NaT\n",
    "\n",
    "# Apply date parsing\n",
    "df_sheet1['Birthday'] = df_sheet1['Birthday'].apply(parse_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bbd1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_sheet2 = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet2')\n",
    "\n",
    "# Apply date parsing\n",
    "df_sheet2['Date'] = df_sheet2['Date'].apply(parse_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1487eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_sheet4 = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet4')\n",
    "\n",
    "# Apply date parsing\n",
    "df_sheet4['Order Date'] = df_sheet4['Order Date'].apply(parse_date)\n",
    "df_sheet4['Delivery Date'] = df_sheet4['Delivery Date'].apply(parse_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e27002ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_sheet5 = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet5')\n",
    "\n",
    "# Apply date parsing\n",
    "df_sheet5['Open Date'] = df_sheet5['Open Date'].apply(parse_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07cca7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data\n",
    "df_sheet1.to_csv('/Users/shadivaz/Desktop/Sheet1_cleaned.csv', index=False)\n",
    "df_sheet2.to_csv('/Users/shadivaz/Desktop/Sheet2_cleaned.csv', index=False)\n",
    "df_sheet4.to_csv('/Users/shadivaz/Desktop/Sheet4_cleaned.csv', index=False)\n",
    "df_sheet5.to_csv('/Users/shadivaz/Desktop/Sheet5_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a42eab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent   Birthday  \n",
      "0    SOUTH AUSTRALIA     5523  Australia  Australia 1939-07-03  \n",
      "1  WESTERN AUSTRALIA     6522  Australia  Australia 1979-09-27  \n",
      "2           VICTORIA     3380  Australia  Australia 1947-05-26  \n",
      "3    SOUTH AUSTRALIA     5223  Australia  Australia 1957-09-17  \n",
      "4           VICTORIA     3698  Australia  Australia 1965-11-19  \n",
      "        Date Currency  Exchange\n",
      "0 2015-01-01      USD    1.0000\n",
      "1 2015-01-01      CAD    1.1583\n",
      "2 2015-01-01      AUD    1.2214\n",
      "3 2015-01-01      EUR    0.8237\n",
      "4 2015-01-01      GBP    0.6415\n",
      "   Order Number  Line Item Order Date Delivery Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1 2016-01-01    2016-01-13       265598        10   \n",
      "1        366001          1 2016-01-01    2016-01-13      1269051         0   \n",
      "2        366001          2 2016-01-01    2016-01-13      1269051         0   \n",
      "3        366002          1 2016-01-01    2016-01-12       266019         0   \n",
      "4        366002          2 2016-01-01    2016-01-12       266019         0   \n",
      "\n",
      "   ProductKey  Quantity Currency Code  \n",
      "0        1304         1           CAD  \n",
      "1        1048         2           USD  \n",
      "2        2007         1           USD  \n",
      "3        1106         7           CAD  \n",
      "4         373         1           CAD  \n",
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0 2008-01-01\n",
      "1         2  Australia            Northern Territory          665.0 2008-01-12\n",
      "2         3  Australia               South Australia         2000.0 2012-01-07\n",
      "3         4  Australia                      Tasmania         2000.0 2010-01-01\n",
      "4         5  Australia                      Victoria         2000.0 2015-12-09\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert Excel serial date to datetime\n",
    "def excel_serial_to_datetime(serial):\n",
    "    # Excel dates start from 1900-01-01, but Excel incorrectly considers 1900 as a leap year\n",
    "    start_date = pd.Timestamp('1899-12-30')  # Correct start date to adjust leap year issue\n",
    "    return start_date + pd.to_timedelta(serial, 'D')\n",
    "\n",
    "# Load the data\n",
    "df_sheet1 = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet1')\n",
    "df_sheet2 = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet2')\n",
    "df_sheet4 = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet4')\n",
    "df_sheet5 = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet5')\n",
    "\n",
    "# Apply conversion for each relevant column\n",
    "df_sheet1['Birthday'] = df_sheet1['Birthday'].apply(lambda x: excel_serial_to_datetime(x) if pd.notnull(x) else np.nan)\n",
    "df_sheet2['Date'] = df_sheet2['Date'].apply(lambda x: excel_serial_to_datetime(x) if pd.notnull(x) else np.nan)\n",
    "df_sheet4['Order Date'] = df_sheet4['Order Date'].apply(lambda x: excel_serial_to_datetime(x) if pd.notnull(x) else np.nan)\n",
    "df_sheet4['Delivery Date'] = df_sheet4['Delivery Date'].apply(lambda x: excel_serial_to_datetime(x) if pd.notnull(x) else np.nan)\n",
    "df_sheet5['Open Date'] = df_sheet5['Open Date'].apply(lambda x: excel_serial_to_datetime(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Check the cleaned data\n",
    "print(df_sheet1.head())\n",
    "print(df_sheet2.head())\n",
    "print(df_sheet4.head())\n",
    "print(df_sheet5.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "839fc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data to CSV\n",
    "df_sheet1.to_csv('/Users/shadivaz/Desktop/Sheet1_cleaned.csv', index=False)\n",
    "df_sheet2.to_csv('/Users/shadivaz/Desktop/Sheet2_cleaned.csv', index=False)\n",
    "df_sheet4.to_csv('/Users/shadivaz/Desktop/Sheet4_cleaned.csv', index=False)\n",
    "df_sheet5.to_csv('/Users/shadivaz/Desktop/Sheet5_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c95b7fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "   Unit Cost USD  Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0           6.62           12.99             101     MP4&MP3            1   \n",
      "1           6.62           12.99             101     MP4&MP3            1   \n",
      "2           7.40           14.52             101     MP4&MP3            1   \n",
      "3          11.00           21.57             101     MP4&MP3            1   \n",
      "4          11.00           21.57             101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Product sheet\n",
    "df_product = pd.read_excel('/Users/shadivaz/Desktop/dataspark copy.xlsx', sheet_name='Sheet3')\n",
    "\n",
    "# Display the first few rows to check the data\n",
    "print(df_product.head())\n",
    "\n",
    "# Save the cleaned Product DataFrame to CSV\n",
    "df_product.to_csv('/Users/shadivaz/Desktop/Product_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba18c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas as pd\n",
    "\n",
    "# SQLAlchemy engine setup\n",
    "engine = create_engine('mysql+mysqlconnector://root:Shadi8903!@localhost/project2')\n",
    "\n",
    "def load_csv_to_sql(csv_path, table_name, engine):\n",
    "    # Create a connection and transaction context\n",
    "    with engine.connect() as connection:\n",
    "        with connection.begin() as transaction:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df.to_sql(table_name, con=connection, if_exists='replace', index=False)\n",
    "                # Commit the transaction\n",
    "                transaction.commit()\n",
    "            except SQLAlchemyError as e:\n",
    "                # Rollback in case of error\n",
    "                transaction.rollback()\n",
    "                print(f\"Error loading {table_name}: {e}\")\n",
    "                raise\n",
    "\n",
    "# File paths and table names\n",
    "csv_files = {\n",
    "    '/Users/shadivaz/Desktop/Sheet1_cleaned.csv': 'Customers',\n",
    "    '/Users/shadivaz/Desktop/Sheet2_cleaned.csv': 'ExchangeRates',\n",
    "    '/Users/shadivaz/Desktop/Sheet4_cleaned.csv': 'Sales',\n",
    "    '/Users/shadivaz/Desktop/Sheet5_cleaned.csv': 'Stores',\n",
    "    '/Users/shadivaz/Desktop/Product_cleaned.csv': 'Products'\n",
    "}\n",
    "\n",
    "# Load each CSV file into SQL\n",
    "for file_path, table_name in csv_files.items():\n",
    "    load_csv_to_sql(file_path, table_name, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758f075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
